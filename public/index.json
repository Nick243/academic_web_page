[{"authors":["admin"],"categories":null,"content":"Nicholas Ollberding is an epidemiologist with applied research interests into the role of diet in the etiology and progression of chronic disease and the impact of the developing infant intestinal microbiome on growth and early development. Areas of methodical research interest include the application and development of analytical methods for microbial metagenomic next-generation sequence data, dietary assessment and analysis methodology, predictive modeling, and casual inference. He also collaborates broadly as a quantitative methodologist in the area of health sciences research and leads the Biostatistics Core of the Heart Institute Research Core at Cincinnati Children’s Hospital Medical Center.\nAll thoughts and opinions expressed here are my own and not the views of Cincinnati Children’s Hospital Medical Center or the University of Cincinnati.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nicholas Ollberding is an epidemiologist with applied research interests into the role of diet in the etiology and progression of chronic disease and the impact of the developing infant intestinal microbiome on growth and early development. Areas of methodical research interest include the application and development of analytical methods for microbial metagenomic next-generation sequence data, dietary assessment and analysis methodology, predictive modeling, and casual inference. He also collaborates broadly as a quantitative methodologist in the area of health sciences research and leads the Biostatistics Core of the Heart Institute Research Core at Cincinnati Children’s Hospital Medical Center.","tags":null,"title":"Nicholas Ollberding","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["Predictive Modeling"],"content":" \nThis is post is to introduce members of the Cincinnati Children’s Hospital Medical Center R Users Group (CCHMC-RUG) to some of the functionality provided by Frank Harrell’s Hmisc and rms packages for data description and predictive modeling. For those of you who have worked with these packages before, hopefully we will cover something new. Dr. Harrell is the founding chair of the Department of Biostatistics at Vanderbilt University (until 2017), Fellow of the American Statistical Association, and a member of the R Foundation. A full biography and list of his numerous accomplishments and honors can be found on his Vanderbilt University webpage.\nI titled the post An Introduction to the Harrell“verse”, because like the tidyverse, these packages are a tremendous resource for the R community; share an underlying grammar and data structure; and reflect the opinioned philosophy of the developer. I also like the term Harrell“verse” because the Hmisc and rms packages link users to a much broader set of materials on modern statistical methods and computing including predictive modeling, estimation, hypothesis testing, and study design. I have no idea how he finds the time to develop these rich and intensive resources, and share his thoughts on social media and elsewhere, but his contributions open-source software and insight into the application of statistical methods are much appreciated! Links to many of these materials can be found at his webpage. A few highlights include:\n His Statistical Thinking blog The Regression Modeling Strategies textbook, courses, and course notes. I have taken both his 1-day and full week courses and highly recommend them! Biostatistics for Biomedical Research e-book datamethods webpage Numerous contributions on stackexchange He also is extremely active on twitter And as of recent, leads a weekly one-hour live webinar on applied statistics/biostatistics  \nMy goal for this RUG session is to briefly introduce you to some of the functionally of the Hmisc and rms packages that I think will be of interest to anyone performing statistical modeling in R. These examples are shown at a high-level and are not meant to demonstrate “best practices” with respect to model building and validation, etc. They just serve to show some of the tools these package can help you bring to bear on a project once working in the Harrell“verse”.\nBelow, I present various functions found in these packages as a series of five tips These “tips” just scratch the surface of the what is possible with Hmisc and rms, but I hope serve to highlight how model complexity can be more easily incorporated when fitting generalized linear models, fitted models visualized, and predictions made and validated when using these packages. Topics covered include:\nExamining your data with Hmisc Regression modeling allowing for complexity with rms Validating fitted models with rms::validate() and rms:calibrate() Penalized regression with rms::pentrace() Models other than OLS for continuous or semi-continuous Y in rms  \nI highly recommend reading Professor Harrell’s Regression Modeling Strategies textbook (2nd edition) and course notes to learn more. Both texts provide detailed code and explanations and Chapter 6 of the textbook is devoted to R and the rms package.\nThe data used in the examples below come from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Wine) and can be accessed via the ucidata R package found on James Balamuta’s github repository. We will be trying to predict whether a wine is red or white from a set of 13 features. A description of the dataset from the UCI website is provided below.\nData Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. I think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.) I lost it, and b.), I would not know which 13 variables are included in the set. The attributes are (dontated by Riccardo Leardi, riclea \u0026#39;@\u0026#39; anchem.unige.it ) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10)Color intensity 11)Hue 12)OD280/OD315 of diluted wines 13)Proline In a classification context, this is a well posed problem with \u0026quot;well behaved\u0026quot; class structures. A good data set for first testing of a new classifier, but not very challenging. Attribute Information: All attributes are continuous  \nInstalling and loading packages #Install ucidata package from github #devtools::install_github(\u0026quot;coatless/ucidata\u0026quot;) #Load packages library(\u0026quot;tidyverse\u0026quot;); packageVersion(\u0026quot;tidyverse\u0026quot;) ## [1] \u0026#39;1.2.1\u0026#39; library(\u0026quot;rms\u0026quot;); packageVersion(\u0026quot;rms\u0026quot;) ## [1] \u0026#39;5.1.3.1\u0026#39; library(\u0026quot;ucidata\u0026quot;); packageVersion(\u0026quot;ucidata\u0026quot;) ## [1] \u0026#39;0.0.3\u0026#39; library(\u0026quot;cowplot\u0026quot;); packageVersion(\u0026quot;cowplot\u0026quot;) ## [1] \u0026#39;1.0.0\u0026#39; \n Loading example dataset Now that the ucidata package is loaded, we can call the wine data set using the base R function data(). We will also create a binary indicator for red_wine that will serve as our outcome variable where “red wine” == 1 and “white wine” == 0. The variable “color” is dropped from the data.frame using dplyr::select().\n#Load wine data data(wine) #Recode outcome mydata \u0026lt;- wine %\u0026gt;% dplyr::mutate(red_wine = ifelse(color == \u0026quot;Red\u0026quot;, 1, 0)) %\u0026gt;% dplyr::select(-color)  \n Tip 1. Examining your data with Hmisc The Hmisc package has some excellent functions to help you understand your data in terms of the distribution of variables, levels of categorical variables, number of missing values, etc. It also has some very useful commands to help visualize this information. Below are a few of the functions I use most often.\n\nHmisc::describe() Describe is a very handy function that allows one to…as you might expect…describe a data.frame. It will provide information on the count, number of missing values, and distinct values, as well as describe and plot the distribution of continuous values. For categorical variables the number of levels and counts are also provided. I find myself using the function as a first pass to look for implausible values in a data.frame, get a feel for the extent of missing data, and to quickly access specific quantiles for a set of predictors. The code below will generate and plot this information. The html options are provided to improve the formatting.\nd \u0026lt;- Hmisc::describe(mydata) html(d, size = 80, scroll = TRUE)   .earrows {color:silver;font-size:11px;} fcap { font-family: Verdana; font-size: 12px; color: MidnightBlue } smg { font-family: Verdana; font-size: 10px; color: \u0026#808080; } hr.thinhr { margin-top: 0.15em; margin-bottom: 0.15em; } span.xscript { position: relative; } span.xscript sub { position: absolute; left: 0.1em; bottom: -1ex; }  mydata 13 Variables 6497 Observations fixed_acidity  .hmisctable441687 { border: none; font-size: 80%; } .hmisctable441687 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable441687 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 649701060.9997.2151.325.76.06.47.07.78.89.8  lowest : 3.8 3.9 4.2 4.4 4.5 , highest: 14.3 15.0 15.5 15.6 15.9 volatile_acidity  .hmisctable354119 { border: none; font-size: 80%; } .hmisctable354119 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable354119 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 649701870.9990.33970.17160.160.180.230.290.400.590.67  lowest : 0.080 0.085 0.090 0.100 0.105 , highest: 1.180 1.185 1.240 1.330 1.580 citric_acid  .hmisctable969301 { border: none; font-size: 80%; } .hmisctable969301 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable969301 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 64970890.9990.31860.15710.050.140.250.310.390.490.56  lowest : 0.00 0.01 0.02 0.03 0.04 , highest: 0.91 0.99 1.00 1.23 1.66 residual_sugar  .hmisctable478822 { border: none; font-size: 80%; } .hmisctable478822 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable478822 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 6497031615.4434.958 1.2 1.3 1.8 3.0 8.113.015.0  lowest : 0.60 0.70 0.80 0.90 0.95 , highest: 22.60 23.50 26.05 31.60 65.80 chlorides  .hmisctable528091 { border: none; font-size: 70%; } .hmisctable528091 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable528091 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 6497021410.056030.028690.0280.0310.0380.0470.0650.0860.102  lowest : 0.009 0.012 0.013 0.014 0.015 , highest: 0.422 0.464 0.467 0.610 0.611 free_sulfur_dioxide  .hmisctable322609 { border: none; font-size: 80%; } .hmisctable322609 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable322609 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 64970135130.5319.51 6 91729415461  lowest : 1.0 2.0 3.0 4.0 5.0 , highest: 128.0 131.0 138.5 146.5 289.0 total_sulfur_dioxide  .hmisctable998318 { border: none; font-size: 80%; } .hmisctable998318 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable998318 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 649702761115.764.31 19 30 77118156188206  lowest : 6.0 7.0 8.0 9.0 10.0 , highest: 307.5 313.0 344.0 366.5 440.0 density  .hmisctable340134 { border: none; font-size: 70%; } .hmisctable340134 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable340134 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 6497099810.99470.0033880.98990.99070.99230.99490.99700.99840.9994  lowest : 0.98711 0.98713 0.98722 0.98740 0.98742 , highest: 1.00315 1.00320 1.00369 1.01030 1.03898 pH  .hmisctable777493 { border: none; font-size: 80%; } .hmisctable777493 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable777493 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 6497010813.2190.18022.973.023.113.213.323.423.50  lowest : 2.72 2.74 2.77 2.79 2.80 , highest: 3.81 3.82 3.85 3.90 4.01 sulphates  .hmisctable604071 { border: none; font-size: 80%; } .hmisctable604071 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable604071 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 649701110.9990.53130.15560.350.370.430.510.600.720.79  lowest : 0.22 0.23 0.25 0.26 0.27 , highest: 1.61 1.62 1.95 1.98 2.00 alcohol  .hmisctable112360 { border: none; font-size: 80%; } .hmisctable112360 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable112360 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 649701110.99910.491.348 9.0 9.1 9.510.311.312.312.7  lowest : 8.00 8.40 8.50 8.60 8.70 , highest: 13.90 14.00 14.05 14.20 14.90 quality  .hmisctable424466 { border: none; font-size: 80%; } .hmisctable424466 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable424466 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoMeanGmd 6497070.8775.8180.9233  Value 3 4 5 6 7 8 9 Frequency 30 216 2138 2836 1079 193 5 Proportion 0.005 0.033 0.329 0.437 0.166 0.030 0.001  red_wine  .hmisctable250749 { border: none; font-size: 80%; } .hmisctable250749 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable250749 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; }  nmissingdistinctInfoSumMeanGmd 6497020.55715990.24610.3711   \nAs you can see, much useful descriptive information is printed to the console. You can plot the histograms for these predictors separately using the plotting commands below.\np \u0026lt;- plot(d) p$Continuous \n Hmisc::summaryM() to obtain a table of predictors stratified by outcome Hmisc::summaryM() summarizes the variables listed in an S formula, computing descriptive statistics and optionally statistical tests for group differences. This function is typically used when there are multiple left-hand-side variables that are independently against by groups marked by a single right-hand-side variable (from help).\nI find summaryM can provide a great “head start” to generating a “Table 1” for a manuscript. However, one should think about the information that is being conveyed when providing a table of unconditional tests via the test = TRUE option. Differences between the unconditional and the conditional estimates can be a source of confusion (or at least may end up requiring a longer than needed explanation) when both are presented. Moreover, one should think hard about whether or not to include formal tests if the study was not specifically designed to test these factors (and whether the p-values actually provide useful information). I am including the test = TRUE option here just to highlight this functionality.\ns \u0026lt;- Hmisc::summaryM(fixed_acidity + volatile_acidity + citric_acid + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide+ density + pH + sulphates + alcohol + quality ~ red_wine, data = mydata, overall = TRUE, test = TRUE, continuous = 5) html(s, caption=\u0026#39;Predictors according to wine type\u0026#39;, exclude1 = TRUE, npct = \u0026#39;both\u0026#39;, digits = 2, prmsd = TRUE, brmsd = TRUE, msdsize = mu$smaller2)  Predictors according to wine type.    0\nN=4898 1\nN=1599 Combined\nN=6497 Test Statistic\n     fixed_acidity 6.30 6.80 7.30\n6.85 ± 0.84 7.10 7.90 9.20\n8.32 ± 1.74 6.40 7.00 7.70\n7.22 ± 1.30 F1 6495=1421, P   volatile_acidity 0.21 0.26 0.32\n0.28 ± 0.10 0.39 0.52 0.64\n0.53 ± 0.18 0.23 0.29 0.40\n0.34 ± 0.16 F1 6495=3637, P   citric_acid 0.27 0.32 0.39\n0.33 ± 0.12 0.09 0.26 0.42\n0.27 ± 0.19 0.25 0.31 0.39\n0.32 ± 0.15 F1 6495=173, P   residual_sugar 1.7 5.2 9.9\n6.4 ± 5.1 1.9 2.2 2.6\n2.5 ± 1.4 1.8 3.0 8.1\n5.4 ± 4.8 F1 6495=458, P   chlorides 0.036 0.043 0.050\n0.046 ± 0.022 0.070 0.079 0.090\n0.087 ± 0.047 0.038 0.047 0.065\n0.056 ± 0.035 F1 6495=5156, P   free_sulfur_dioxide 23 34 46\n35 ± 17 7 14 21\n16 ± 10 17 29 41\n31 ± 18 F1 6495=2409, P   total_sulfur_dioxide 108 134 167\n138 ± 42 22 38 62\n46 ± 33 77 118 156\n116 ± 57 F1 6495=5473, P   density 0.9917 0.9937 0.9961\n0.9940 ± 0.0030 0.9956 0.9968 0.9978\n0.9967 ± 0.0019 0.9923 0.9949 0.9970\n0.9947 ± 0.0030 F1 6495=1300, P   pH 3.09 3.18 3.28\n3.19 ± 0.15 3.21 3.31 3.40\n3.31 ± 0.15 3.11 3.21 3.32\n3.22 ± 0.16 F1 6495=829, P   sulphates 0.41 0.47 0.55\n0.49 ± 0.11 0.55 0.62 0.73\n0.66 ± 0.17 0.43 0.51 0.60\n0.53 ± 0.15 F1 6495=2101, P   alcohol 9.5 10.4 11.4\n10.5 ± 1.2 9.5 10.2 11.1\n10.4 ± 1.1 9.5 10.3 11.3\n10.5 ± 1.2 F1 6495=1.8, P=0.18   quality 5.00 6.00 6.00\n5.88 ± 0.89 5.00 6.00 6.00\n5.64 ± 0.81 5.00 6.00 6.00\n5.82 ± 0.87 F1 6495=100, P   a b c represent the lower quartile a, the median b, and the upper quartile c for continuous variables. x ± s represents X ± 1 SD. Test used: Wilcoxon test .  \n Spike histograms with Hmisc::histSpikeg(), ggplot, and cowplot Dr. Harrell’s histSpikeg function provides a very powerful approach to visualize the univariable association between a continuous predictor and binary outcome. It does this by binning the continuous x variable into equal-width bins and then computing and plotting the frequency counts of Y within each bin. The function then displays the proportions as a vertical histogram with a lowess curve fit to the plot. histSpikeg allows this functionality to be added as a ggplot layer.\nHere I am combing the plots into a single figure using Claus Wilke’s cowplot package plot_grid() function.\ndd \u0026lt;- datadist(mydata) options(datadist = \u0026quot;dd\u0026quot;) a \u0026lt;- ggplot(mydata, aes(x = alcohol, y = red_wine)) + Hmisc::histSpikeg(red_wine ~ alcohol, lowess = TRUE, data = mydata) + labs(x = \u0026quot;\\nAlcohol Content\u0026quot;, y = \u0026quot;Probability(Red Wine)\\n\u0026quot;) b \u0026lt;- ggplot(mydata, aes(x = citric_acid, y = red_wine)) + Hmisc::histSpikeg(red_wine ~ citric_acid, lowess = TRUE, data = mydata) + labs(x = \u0026quot;\\nCitric Acid\u0026quot;, y = \u0026quot;Probability(Red Wine)\\n\u0026quot;) c \u0026lt;- ggplot(mydata, aes(x = sulphates, y = red_wine)) + Hmisc::histSpikeg(red_wine ~ sulphates, lowess = TRUE, data = mydata) + labs(x = \u0026quot;\\nSulphates\u0026quot;, y = \u0026quot;Probability(Red Wine)\\n\u0026quot;) cowplot::plot_grid(a, b, c, nrow = 1, ncol = 3, scale = .9, labels = \u0026quot;AUTO\u0026quot;) \nThese plots suggest that allowing for complexity/flexibly of the right hand side of the linear model equation may provide improved performance. However, Dr. Harrell has argued (convincingly IMO) that including non-linear terms for continuous predictors should typically be default practice since 1.) truly linear functional forms are probably the expectation more so than the rule (at least for most biomedical phenomena) and 2.) you will likely have more success and “miss small” when including non-linear terms if the true association happens to be linear than if the true association is highly non-linear and one models the association with a simple linear term (i.e. fits a linear term to a u-shaped association diminishing model performance). The cost, of course, is a few model degrees of freedom and the possibility for overfitting…which we will come back to latter.\n\n Visualizing patterns of missing data with Hmisc::naplot and Hmisc::naclus While there are no missing values in these data, the Hmisc package has several nice features to assess and visualize the extent of missing data and patterns of missing data among variables. Here we will use the dplyr::mutate() function to set some of the values to missing. Then the naplot function is used to plot the proportion of missing values for each variable in the dataset and the naclus function is used to assess patterns of “missingness” among variables. This information can be very helpful to understand why values might be missing and to inform imputation strategies.\nWhile there is no need to impute missing values in this example, the Hmisc:: aregImpute function provides a rigorous approach to handling missing data via multiple imputation using additive regression with various options for bootstrapping, predictive mean matching, etc. Multiple imputations can then be properly combined using Rubin’s rules via the fit.mult.impute function. This function can also be used with output from Stef van Buuren’s MICE package.\nmissing_df \u0026lt;- mydata %\u0026gt;% dplyr::mutate(fixed_acidity = ifelse(row_number() %in% c(1:100), NA, fixed_acidity), volatile_acidity = ifelse(row_number() %in% c(1:200), NA, volatile_acidity), citric_acid = ifelse(row_number() %in% c(50:400), NA, citric_acid), residual_sugar = ifelse(row_number() %in% c(1000:1050), NA, residual_sugar), chlorides = ifelse(row_number() %in% c(1000:1100), NA, chlorides)) par(mfrow = c(1,2)) na_patterns \u0026lt;- Hmisc::naclus(missing_df) Hmisc::naplot(na_patterns, \u0026#39;na per var\u0026#39;) plot(na_patterns) \n  Tip 2. Regression modeling allowing for complexity with rms \nThe Hmisc and rms packages were recommend to me some time ago when I was looking for better approaches (than say base R functions…as good as they are) to incorporate non-linear terms and interactions into a generalized linear model framework. The rms package makes adding such complexity extremely accessible with reasonable defaults. Professor Harrell discusses the use of restricted cubic spline terms (natural splines) to relax the assumption of linearity for continuous predictors, as well as some of the advantages of constraining the spline function to be linear in the tails in Chapter 2 of the second edition of his Regression Modeling Stratigies textbook. He also provides a detailed description regarding the number and placement of knots that I found quite informative. I typically stick with the recommend default knot placement unless I have good reason to do otherwise, but alternative placement can be easily accommodated.\nThe rms package also eases the programming required to fit and test interaction terms (using anova.rms) and recognizes syntax of the forms below which were taken from Cole Beck’s rms tutorial:\n y ~ a:b, : indicates the interaction of a and b y ~ a*b, equivalent to y ~ a+b+a:b y ~ (a+b)^2, equivalent to y ~ (a+b)*(a+b) and the restricted interaction term %ia% that for non-linear predictors is not doubly nonlinear in the interaction  The code below will fit an additive model to the full set of predictors allowing for flexibility using restricted cubic splines fit via the rms::rcs() function.\n\nm0 \u0026lt;- lrm(red_wine ~ rcs(fixed_acidity, 4) + rcs(volatile_acidity, 4) + rcs(citric_acid, 4) + rcs(residual_sugar, 4) + rcs(chlorides, 4) + rcs(free_sulfur_dioxide, 4) + rcs(total_sulfur_dioxide, 4) + rcs(density, 4) + rcs(pH, 4) + rcs(sulphates, 4) + rcs(alcohol, 4) + rcs(quality, 3), data = mydata, x = TRUE, y = TRUE) print(m0, coef = FALSE) ## Logistic Regression Model ## ## lrm(formula = red_wine ~ rcs(fixed_acidity, 4) + rcs(volatile_acidity, ## 4) + rcs(citric_acid, 4) + rcs(residual_sugar, 4) + rcs(chlorides, ## 4) + rcs(free_sulfur_dioxide, 4) + rcs(total_sulfur_dioxide, ## 4) + rcs(density, 4) + rcs(pH, 4) + rcs(sulphates, 4) + rcs(alcohol, ## 4) + rcs(quality, 3), data = mydata, x = TRUE, y = TRUE) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 7008.47 R2 0.981 C 0.999 ## 0 4898 d.f. 35 g 8.879 Dxy 0.997 ## 1 1599 Pr(\u0026gt; chi2) \u0026lt;0.0001 gr 7178.012 gamma 0.998 ## max |deriv| 1e-05 gp 0.371 tau-a 0.370 ## Brier 0.004 plot(anova(m0)) So…it looks like we can rather easily predict the type of wine from this set of features! If I would have known this, I might have selected a more difficult example problem. I suppose if I did not know at least this much, my this a second career as a sommelier is probably out of the question at this point!\nOur next steps would typically be to validate the model and assess how well it is calibrated over the range of predictions, but here I want to continue an example that is a bit more similar to what I commonly see in practice (i.e. we have a set of features with moderate predictive performance). I also want to trim down the number of predictors to highlight some of the plotting functions.\nTherefore, I will proceed using only two predictors that have relatively small conditional chi-square values (since even the top few predictors have excellent performance) to show some of the key functionality. The first model is a flexible additive model allowing for 3 knots for each predictor.\nm2 \u0026lt;- lrm(red_wine ~ rcs(pH, 3) + rcs(sulphates, 3), data = mydata, x = TRUE, y = TRUE) print(m2) ## Logistic Regression Model ## ## lrm(formula = red_wine ~ rcs(pH, 3) + rcs(sulphates, 3), data = mydata, ## x = TRUE, y = TRUE) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 2294.01 R2 0.442 C 0.866 ## 0 4898 d.f. 4 g 2.415 Dxy 0.731 ## 1 1599 Pr(\u0026gt; chi2) \u0026lt;0.0001 gr 11.186 gamma 0.731 ## max |deriv| 7e-07 gp 0.271 tau-a 0.271 ## Brier 0.125 ## ## Coef S.E. Wald Z Pr(\u0026gt;|Z|) ## Intercept -40.0731 2.4341 -16.46 \u0026lt;0.0001 ## pH 8.4567 0.7536 11.22 \u0026lt;0.0001 ## pH\u0026#39; -4.3193 0.7323 -5.90 \u0026lt;0.0001 ## sulphates 23.5236 1.3365 17.60 \u0026lt;0.0001 ## sulphates\u0026#39; -15.9641 1.3353 -11.96 \u0026lt;0.0001 ##  anova(m2) ## Wald Statistics Response: red_wine ## ## Factor Chi-Square d.f. P ## pH 347.99 2 \u0026lt;.0001 ## Nonlinear 34.79 1 \u0026lt;.0001 ## sulphates 920.43 2 \u0026lt;.0001 ## Nonlinear 142.93 1 \u0026lt;.0001 ## TOTAL NONLINEAR 185.52 2 \u0026lt;.0001 ## TOTAL 1205.84 4 \u0026lt;.0001 p1 \u0026lt;- ggplot(Predict(m2, pH)) p2 \u0026lt;- ggplot(Predict(m2, sulphates)) p3 \u0026lt;- ggplot(Predict(m2, pH, fun = plogis)) p4 \u0026lt;- ggplot(Predict(m2, sulphates, fun = plogis)) cowplot::plot_grid(p1, p2, p3, p4, nrow = 2, ncol = 2, scale = .9) The Brier score and C-statistic suggest possible moderate predictive performance Printing the model also provides information and tests for the coefficients and non-linear terms (which may or may not be informative depending on your goals). The anova.rms function is quite impressive and automatically tests the most meaningful hypotheses in a design including Wald tests for the non-linear terms, and interactions if they were included (as we will see below), as well as chunk tests for any set of terms of interest. Here is a link with an example of the chunk test on stackexchange.\nThe Predict function selects plausible values to provide predictions over and can be plotted using ggplot. By default, the predictions for rms::lrm() are returned on the log-odds scale. If we provide a function to the fun = option, we can obtain the predictions on an alternative scale. Here we use the plogis function to return the predicted probabilities. These are calculated at the median value for continuous variables and at the most common level for categorical variables by default (but can be changed).\nBelow I use the ^2 operator to include all two-way interactions in the model (in this simple example this is a bit of overkill and other choices of syntax used). The rms::bplot() function is very cool and allows one to visualize the two-way interaction. The last line of code highlights how specific values of a feature can be provided to the rms::Predict () function. This is incredibly useful when you want to examine how multiple factors are operating. Dr. Harrell provides many example of this in his RMS textbook.\nm3 \u0026lt;- lrm(red_wine ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, data = mydata, x = TRUE, y = TRUE) print(m3, coef = FALSE) ## Logistic Regression Model ## ## lrm(formula = red_wine ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, ## data = mydata, x = TRUE, y = TRUE) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 2333.97 R2 0.449 C 0.867 ## 0 4898 d.f. 8 g 2.579 Dxy 0.734 ## 1 1599 Pr(\u0026gt; chi2) \u0026lt;0.0001 gr 13.181 gamma 0.734 ## max |deriv| 2e-09 gp 0.272 tau-a 0.272 ## Brier 0.123 anova(m3) ## Wald Statistics Response: red_wine ## ## Factor Chi-Square d.f. P ## pH (Factor+Higher Order Factors) 355.42 6 \u0026lt;.0001 ## All Interactions 35.00 4 \u0026lt;.0001 ## Nonlinear (Factor+Higher Order Factors) 46.09 3 \u0026lt;.0001 ## sulphates (Factor+Higher Order Factors) 887.41 6 \u0026lt;.0001 ## All Interactions 35.00 4 \u0026lt;.0001 ## Nonlinear (Factor+Higher Order Factors) 161.97 3 \u0026lt;.0001 ## pH * sulphates (Factor+Higher Order Factors) 35.00 4 \u0026lt;.0001 ## Nonlinear 6.41 3 0.0933 ## Nonlinear Interaction : f(A,B) vs. AB 6.41 3 0.0933 ## f(A,B) vs. Af(B) + Bg(A) 0.06 1 0.8011 ## Nonlinear Interaction in pH vs. Af(B) 2.36 2 0.3073 ## Nonlinear Interaction in sulphates vs. Bg(A) 2.94 2 0.2297 ## TOTAL NONLINEAR 204.16 5 \u0026lt;.0001 ## TOTAL NONLINEAR + INTERACTION 214.89 6 \u0026lt;.0001 ## TOTAL 1159.86 8 \u0026lt;.0001 pred_intx \u0026lt;- Predict(m3, \u0026#39;pH\u0026#39;,\u0026#39;sulphates\u0026#39;, fun = plogis, np = 75) bplot(pred_intx, yhat ~ pH + sulphates, lfun = wireframe, ylab = \u0026quot;Sulphates\u0026quot;, zlab = \u0026quot;Pr(Red Wine)\\n\u0026quot;) ggplot(Predict(m2, sulphates, pH = c(3.0, 3.2, 3.4), fun = plogis)) \nThe inclusion of the interaction does not look to add much in term of the absolute or rank-based discrimination. However, this example highlights the various types of tests that anova.rms performs by default. It also allows us to see the response profile surface returned by bplot().\nLet’s now go ahead and fit the restricted interaction just for instructive purposes.\n#%ia% is restricted interaction - not doubly nonlinear m4 \u0026lt;- lrm(red_wine ~ rcs(pH, 3) + rcs(sulphates, 3) + pH %ia% sulphates, data = mydata, x = TRUE, y = TRUE) print(m4, coef = FALSE) ## Logistic Regression Model ## ## lrm(formula = red_wine ~ rcs(pH, 3) + rcs(sulphates, 3) + pH %ia% ## sulphates, data = mydata, x = TRUE, y = TRUE) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 2327.12 R2 0.448 C 0.866 ## 0 4898 d.f. 5 g 2.657 Dxy 0.733 ## 1 1599 Pr(\u0026gt; chi2) \u0026lt;0.0001 gr 14.252 gamma 0.733 ## max |deriv| 2e-11 gp 0.272 tau-a 0.272 ## Brier 0.124 anova(m4) ## Wald Statistics Response: red_wine ## ## Factor Chi-Square d.f. P ## pH (Factor+Higher Order Factors) 362.84 3 \u0026lt;.0001 ## All Interactions 32.25 1 \u0026lt;.0001 ## Nonlinear 44.67 1 \u0026lt;.0001 ## sulphates (Factor+Higher Order Factors) 883.81 3 \u0026lt;.0001 ## All Interactions 32.25 1 \u0026lt;.0001 ## Nonlinear 154.96 1 \u0026lt;.0001 ## pH * sulphates (Factor+Higher Order Factors) 32.25 1 \u0026lt;.0001 ## TOTAL NONLINEAR 198.98 2 \u0026lt;.0001 ## TOTAL NONLINEAR + INTERACTION 205.24 3 \u0026lt;.0001 ## TOTAL 1137.05 5 \u0026lt;.0001 pred_intx_r \u0026lt;- Predict(m4, \u0026#39;pH\u0026#39;,\u0026#39;sulphates\u0026#39;, fun = plogis, np = 75) bplot(pred_intx_r, yhat ~ pH + sulphates, lfun = wireframe, ylab = \u0026quot;Sulphates\u0026quot;, zlab = \u0026quot;Pr(Red Wine)\\n\u0026quot;) \nHere we can see that the results are generally unchanged; however, the restricted interaction and test is for 1 d.f.; whereas, the interaction for the cross-product term with 3 knots in each of the predictors requires 4 d.f.\nThe summary.rms() function can be used to obtain the log-odds and exponentiated odds ratios for each predictor. The interquartile odds ratios are provided by default. These can be easily changed, and more interesting or complex associations tested.\nAs with other GLMs, the rms fitted values of the rms::lrm() object can be obtained for all observations using the predict function.\nsummary(m4) ## Effects Response : red_wine ## ## Factor Low High Diff. Effect S.E. Lower 0.95 Upper 0.95 ## pH 3.11 3.32 0.21 1.4116 0.084427 1.2461 1.5770 ## Odds Ratio 3.11 3.32 0.21 4.1024 NA 3.4767 4.8406 ## sulphates 0.43 0.60 0.17 2.7986 0.124910 2.5538 3.0434 ## Odds Ratio 0.43 0.60 0.17 16.4220 NA 12.8560 20.9770 ## ## Adjusted to: pH=3.21 sulphates=0.51 summary(m4, pH = c(2.97, 3.50)) #contrast of 5th verus 95th %tile ## Effects Response : red_wine ## ## Factor Low High Diff. Effect S.E. Lower 0.95 Upper 0.95 ## pH 2.97 3.5 0.53 3.4147 0.19978 3.0231 3.8062 ## Odds Ratio 2.97 3.5 0.53 30.4070 NA 20.5550 44.9800 ## sulphates 0.43 0.6 0.17 2.7986 0.12491 2.5538 3.0434 ## Odds Ratio 0.43 0.6 0.17 16.4220 NA 12.8560 20.9770 ## ## Adjusted to: pH=3.21 sulphates=0.51 r \u0026lt;- mydata r$fitted \u0026lt;- predict(m4, type = \u0026quot;fitted\u0026quot;) head(r$fitted) ## [1] 0.5605529 0.5164138 0.5470583 0.2846203 0.5605529 0.5605529 \n Tip 3. Validating fitted models with rms::validate() and rms:calibrate() \nIn Chapter 5 of the RMS textbook, the bootstrap procure is advocated for obtaining nearly unbiased estimates of a model’s future performance using resampling. The rms::validate() function implements this procedure to return bias-corrected indexes that are specific to each type of model. There are many indices of performance that are returned! Use ?validate.lrm to get further information on each metric. The steps performed to obtain optimism correct estimates based on bootstrap resampling are:\n Estimate model performance in the original sample of size n Draw a bootstrap sample of the same size n and fit the model to the bootstrap sample Apply the model obtained in the bootstrap sample to the original sample Subtract the accuracy measure found in the bootstrap sample from the accuracy measure in the original sample - this is the estimate of optimism (i.e. overfitting) Repeat the process many times and average over the repeats to obtain a final estimate of optimism for each measure Subtract that value from the observed/apparent accuracy measure to get the optimism corrected estimate  Alternative approaches such as cross-validation or .632 resampling can also be implemented. Below is an example using 200 bootstrap resamples. The second line of code computes the C-statistic (a.k.a. area under the ROC curve) from Somer’s D. There is a lot going on here…but, the validate function abstracts most of it away!\n\n(val \u0026lt;- validate(m4, B = 200)) ## index.orig training test optimism index.corrected n ## Dxy 0.7329 0.7341 0.7325 0.0016 0.7313 200 ## R2 0.4477 0.4493 0.4471 0.0022 0.4455 200 ## Intercept 0.0000 0.0000 -0.0023 0.0023 -0.0023 200 ## Slope 1.0000 1.0000 0.9941 0.0059 0.9941 200 ## Emax 0.0000 0.0000 0.0017 0.0017 0.0017 200 ## D 0.3580 0.3595 0.3574 0.0021 0.3559 200 ## U -0.0003 -0.0003 0.0000 -0.0003 0.0000 200 ## Q 0.3583 0.3599 0.3574 0.0024 0.3559 200 ## B 0.1237 0.1233 0.1238 -0.0005 0.1242 200 ## g 2.6569 2.6652 2.6483 0.0170 2.6400 200 ## gp 0.2719 0.2722 0.2717 0.0005 0.2714 200 (c_opt_corr \u0026lt;- 0.5 * (val[1, 5] + 1)) ## [1] 0.8656726 \nHere we see little evidence of overfitting. This would be expected given the large sample size and small model d.f. However, the difference between the apparent and optimism-corrected estimates can be quite large when estimating complex models on small data sets; suggesting the potential for worse performance when fit to new data. Good information on the number of subjects required to fit predictive models for continuous and binary outcomes to limit overfitting can be found in this excellent set of papers by Richard Riley and colleagues.\nCalibration is an integral component of model validation and aims to gauge how well the model predictions fit observed data (over the full range of values). Bootstrap resampling can be used to obtain out-of-sample estimates of model performance for calibration as well.\nrms::calibrate() uses bootstrapping or cross-validation to get bias-corrected (overfitting-corrected) estimates of predicted vs. observed values based on sub-setting predictions over a sequence of predicted values. The function shows the ideal, apparent, and optimism-corrected calibration curves. It also provides a histogram highlighting the density of predictions.\nHere is an example.\ncal \u0026lt;- calibrate(m4, B = 200) plot(cal) ## ## n=6497 Mean absolute error=0.003 Mean squared error=1e-05 ## 0.9 Quantile of absolute error=0.007 \nThe model looks to fit well over the range of predicted probabilities. Thus, given the limited optimism, and excellent calibration, we might expect this model to perform well in a new sample.\n\n Tip 4. Penalized regression with rms::pentrace() \nPenalized regression can be used to improve the performance of a model when fit to new data by reducing the impact of extreme coefficients. This is a form of bias-variance trade-off off where we can downwardly bias the coefficients to improve the error in new data. Penalized regression is similar to ridge regression in that it is an “L-2” penalty that leaves all terms in the model, but shrinks them towards zero. It is implemented here using penalized maximum likelihood estimation. For those of you familiar with Bayesian statistics, this approach can be thought of as a “frequentist way” to bring in the idea of a “skeptical prior” into the model building exercise. The less weight we want to assign to the raw coefficients, the more we can shrink them. The benefits being potentially improved predictions in new data and reduced effective degrees of freedom. However, there is some work to from Ben Van Calster, Maarten van Smeden, Ewout W. Steyerberg for example, that suggest that while shrinkage improves predictions on average, it can perform poorly in individual datasets and does not typically solve problems associated with small sample size or low number of events per variable. So it is not a panacea for not collecting enough data.\nA particularly useful function in rms::pentrace() is that the main effect terms can receive different penalties (shrinkage factors) than, for example, the non-linear terms or interactions. Thus, it provides a very nice approach to allow for model complexity, while shrinking some/all estimates depending on need.\nWhile for these data little adjustment for overfitting is needed, we will apply pentrace function for instructional purposes. Here the AIC.c is sought to be maximized and the penalty value identified via a grid search. We see that little penalization is required to achieve the maximum AIC.c and that the effective d.f. are reduced in the penalized models.\n\npentrace(m4, seq(.01, .1, by = .01)) ## ## Best penalty: ## ## penalty df ## 0.01 4.981253 ## ## penalty df aic bic aic.c ## 0.00 5.000000 2317.119 2283.224 2317.110 ## 0.01 4.981253 2317.137 2283.369 2317.128 ## 0.02 4.963194 2317.118 2283.472 2317.109 ## 0.03 4.945779 2317.066 2283.538 2317.057 ## 0.04 4.928972 2316.984 2283.570 2316.975 ## 0.05 4.912738 2316.876 2283.572 2316.867 ## 0.06 4.897045 2316.743 2283.546 2316.735 ## 0.07 4.881863 2316.590 2283.495 2316.581 ## 0.08 4.867167 2316.417 2283.422 2316.408 ## 0.09 4.852930 2316.227 2283.328 2316.218 ## 0.10 4.839131 2316.021 2283.217 2316.013 m5 \u0026lt;- update(m4, penalty = .01) m5 ## Logistic Regression Model ## ## lrm(formula = red_wine ~ rcs(pH, 3) + rcs(sulphates, 3) + pH %ia% ## sulphates, data = mydata, x = TRUE, y = TRUE, penalty = 0.01) ## ## ## Penalty factors ## ## simple nonlinear interaction nonlinear.interaction ## 0.01 0.01 0.01 0.01 ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 2327.10 R2 0.448 C 0.866 ## 0 4898 d.f. 4.981 g 2.649 Dxy 0.733 ## 1 1599 Pr(\u0026gt; chi2)\u0026lt;0.0001 gr 14.138 gamma 0.733 ## max |deriv| 1e-11 Penalty 1.15 gp 0.272 tau-a 0.272 ## Brier 0.124 ## ## Coef S.E. Wald Z Pr(\u0026gt;|Z|) Penalty Scale ## Intercept -63.7296 4.9458 -12.89 \u0026lt;0.0001 0.0000 ## pH 15.5212 1.4868 10.44 \u0026lt;0.0001 0.0161 ## pH\u0026#39; -5.0190 0.7531 -6.66 \u0026lt;0.0001 0.0140 ## sulphates 58.7949 6.4401 9.13 \u0026lt;0.0001 0.0149 ## sulphates\u0026#39; -17.6434 1.4195 -12.43 \u0026lt;0.0001 0.0127 ## pH * sulphates -10.3680 1.8527 -5.60 \u0026lt;0.0001 0.0499 ##  \npentrace(m4, list(simple = 0.01, nonlinear = c(0, 0.01, 0.02, 0.03), interaction = c(0, 0.01, 0.02, 0.03))) ## ## Best penalty: ## ## simple nonlinear interaction df ## 0.01 0.02 0.02 4.972499 ## ## simple nonlinear interaction df aic bic aic.c ## 0.01 0.01 0.01 4.981253 2317.137 2283.369 2317.128 ## 0.01 0.01 0.02 4.972933 2317.139 2283.427 2317.130 ## 0.01 0.02 0.02 4.972499 2317.139 2283.430 2317.130 ## 0.01 0.01 0.03 4.964759 2317.136 2283.479 2317.127 ## 0.01 0.02 0.03 4.964326 2317.136 2283.483 2317.127 ## 0.01 0.03 0.03 4.963893 2317.137 2283.486 2317.127 m6 \u0026lt;- update(m4, penalty = .01) m6 ## Logistic Regression Model ## ## lrm(formula = red_wine ~ rcs(pH, 3) + rcs(sulphates, 3) + pH %ia% ## sulphates, data = mydata, x = TRUE, y = TRUE, penalty = 0.01) ## ## ## Penalty factors ## ## simple nonlinear interaction nonlinear.interaction ## 0.01 0.01 0.01 0.01 ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 2327.10 R2 0.448 C 0.866 ## 0 4898 d.f. 4.981 g 2.649 Dxy 0.733 ## 1 1599 Pr(\u0026gt; chi2)\u0026lt;0.0001 gr 14.138 gamma 0.733 ## max |deriv| 1e-11 Penalty 1.15 gp 0.272 tau-a 0.272 ## Brier 0.124 ## ## Coef S.E. Wald Z Pr(\u0026gt;|Z|) Penalty Scale ## Intercept -63.7296 4.9458 -12.89 \u0026lt;0.0001 0.0000 ## pH 15.5212 1.4868 10.44 \u0026lt;0.0001 0.0161 ## pH\u0026#39; -5.0190 0.7531 -6.66 \u0026lt;0.0001 0.0140 ## sulphates 58.7949 6.4401 9.13 \u0026lt;0.0001 0.0149 ## sulphates\u0026#39; -17.6434 1.4195 -12.43 \u0026lt;0.0001 0.0127 ## pH * sulphates -10.3680 1.8527 -5.60 \u0026lt;0.0001 0.0499 ##  effective.df(m6) ## ## Original and Effective Degrees of Freedom ## ## Original Penalized ## All 5 4.98 ## Simple Terms 2 1.99 ## Interaction or Nonlinear 3 2.99 ## Nonlinear 2 2.00 ## Interaction 1 0.99 ## Nonlinear Interaction 0 0.00 \n Tip 5. Models other than OLS for continuous or semi-continuous Y \nA nice feature of the rms package is that one can use it to fit a wide range of models. For example, if we want to make predictions regarding a conditional quantile, rms wraps Roger Koenker’s quantreg package allowing for most of the benefits of the rms package to applied to quantile regression models. The only limitation I am aware of is that when using rms one can only model a single value of tau (quantile) at a time. If one wishes to model ordinal or semi-continuous data, the rms::orm() function fits ordinal cumulative probability models for continuous or ordinal response variables. In addition, the package can be used to fit OLS regression, survival models, generalized least squares for longitudinal data, etc.\nModels other than OLS may come in handy when modeling a continuous outcome and one want to make “less restrictive” assumptions regarding the distribution of Y given X. Quantile regression only requires that Y|X be continuous. The proportional odds model only assumes that the association is the same for all outcome groups (i.e. proportional odds or parallel regression assumption). So, if one does not want to rely on the central limit theorem when making predictions, these alternative approaches can be considered depending on your goals.\nBelow we fit a fairly simple two-term model with non-linear terms and interactions to predict values of residual sugar from pH and sulphates. OLS, ordinal, and quantile regression models are fit to the data and various predictions made.\nlm1 \u0026lt;- ols(residual_sugar ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, data = mydata, x = TRUE, y = TRUE) print(lm1, coefs = FALSE) ## Linear Regression Model ## ## ols(formula = residual_sugar ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, ## data = mydata, x = TRUE, y = TRUE) ## ## Model Likelihood Discrimination ## Ratio Test Indexes ## Obs 6497 LR chi2 710.48 R2 0.104 ## sigma4.5074 d.f. 8 R2 adj 0.102 ## d.f. 6488 Pr(\u0026gt; chi2) 0.0000 g 1.719 ## ## Residuals ## ## Min 1Q Median 3Q Max ## -9.967 -3.202 -1.174 2.444 62.425 ##  r \u0026lt;- mydata r$resid \u0026lt;- resid(lm1) r$fitted \u0026lt;- fitted(lm1) r1 \u0026lt;- ggplot(data = r, aes(x = fitted, y = resid)) + geom_point() + geom_smooth() r2 \u0026lt;- ggplot(data = r, aes(x = pH, y = resid)) + geom_point() + geom_smooth() r3 \u0026lt;- ggplot(data = r, aes(x = sulphates, y = resid)) + geom_point() + geom_smooth() r4 \u0026lt;- ggplot(data = r, aes(sample = resid)) + stat_qq() + geom_abline(intercept = mean(r$resid), slope = sd(r$resid)) cowplot::plot_grid(r1, r2, r3, r4, nrow = 2, ncol = 2, scale = .9) ggplot(Predict(lm1, sulphates, pH = c(3, 3.2, 3.4))) \nWe can see above that neither the r-squared value nor the model fit is great. However, the effect of pH and sulphates does look to interact in a non-linear manner when predicting residual sugar. We could further examine the model predictions at this time or consider some type of transformation, etc. or alternative model.\nWe will fit a log-log proportional odds model as implemented by rms::orm() to these data and obtain mean and median predictions for residual sugar as a function of pH and sulphates.\norm1 \u0026lt;- orm(residual_sugar ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, data = mydata, x = TRUE, y = TRUE) print(orm1, coefs = FALSE) ## Logistic (Proportional Odds) Ordinal Regression Model ## ## orm(formula = residual_sugar ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, ## data = mydata, x = TRUE, y = TRUE) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 6497 LR chi2 476.63 R2 0.071 rho 0.268 ## Distinct Y 316 d.f. 8 g 0.542 ## Median Y 3 Pr(\u0026gt; chi2) \u0026lt;0.0001 gr 1.720 ## max |deriv| 2e-05 Score chi2 482.68 |Pr(Y\u0026gt;=median)-0.5| 0.098 ## Pr(\u0026gt; chi2) \u0026lt;0.0001 M \u0026lt;- Mean(orm1) qu \u0026lt;- Quantile(orm1) med \u0026lt;- function(x) qu(0.5, x) p1 \u0026lt;- ggplot(Predict(orm1, sulphates, pH = c(3, 3.2, 3.4), fun = M)) + coord_cartesian(ylim = c(1, 8)) p2 \u0026lt;- ggplot(Predict(orm1, sulphates, pH = c(3, 3.2, 3.4), fun = med)) + coord_cartesian(ylim = c(1, 8)) plot_grid(p1, p2, nrow = 1, ncol = 2, scale = 0.9, labels = c(\u0026quot;ORM: Mean\u0026quot;, \u0026quot;ORM: Median\u0026quot;)) \nWe generally see the same pattern as we did with OLS.\nNo lets try the rms::Rq function to predcit the median value from this same set of predictors.\nrq1 \u0026lt;- Rq(residual_sugar ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, data = mydata, x = TRUE, y = TRUE, tau = 0.5) print(rq1, coefs = FALSE) ## Quantile Regression tau: 0.5 ## ## Rq(formula = residual_sugar ~ (rcs(pH, 3) + rcs(sulphates, 3))^2, ## tau = 0.5, data = mydata, x = TRUE, y = TRUE) ## ## Discrimination ## Index ## Obs 6497 g 1.971 ## p 9 ## Residual d.f. 6488 ## mean |Y-Yhat| 3.345058 ggplot(Predict(rq1, sulphates, pH = c(3, 3.2, 3.4))) \nAt this point we could compare the optimism-corrected estimates of model performance and the calibration curves to assess performance. We could also model other quantiles by changing tau to see if the impact of the predictors differs across quantiles of residual sugar.\n\n Bonus: taking predictions outside of R \nIf you want to take your predictions outside of R, to drop them into a java script for web-based visualization for example, the rms::Function() will output the R code used to make the model predictions.\n\n(pred_logit \u0026lt;- Function(m4)) ## function (pH = 3.21, sulphates = 0.51) ## { ## -64.339385 + 15.699039 * pH - 31.487899 * pmax(pH - 3.02, ## 0)^3 + 59.976951 * pmax(pH - 3.21, 0)^3 - 28.489052 * ## pmax(pH - 3.42, 0)^3 + 59.695226 * sulphates - 144.55604 * ## pmax(sulphates - 0.37, 0)^3 + 240.92673 * pmax(sulphates - ## 0.51, 0)^3 - 96.370692 * pmax(sulphates - 0.72, 0)^3 - ## 10.624164 * pH * sulphates ## } ## \u0026lt;environment: 0x7ff2193b0d60\u0026gt; \n Session Info \nsessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.5 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] cowplot_1.0.0 ucidata_0.0.3 rms_5.1-3.1 ## [4] SparseM_1.77 Hmisc_4.2-0 Formula_1.2-3 ## [7] survival_2.44-1.1 lattice_0.20-38 forcats_0.4.0 ## [10] stringr_1.4.0 dplyr_0.8.0.1 purrr_0.3.2 ## [13] readr_1.3.1 tidyr_0.8.3 tibble_2.1.1 ## [16] ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.0 jsonlite_1.6 splines_3.6.0 ## [4] modelr_0.1.4 assertthat_0.2.1 latticeExtra_0.6-28 ## [7] cellranger_1.1.0 yaml_2.2.0 pillar_1.3.1 ## [10] backports_1.1.4 quantreg_5.38 glue_1.3.1 ## [13] digest_0.6.18 RColorBrewer_1.1-2 checkmate_1.9.3 ## [16] rvest_0.3.3 colorspace_1.4-1 sandwich_2.5-1 ## [19] htmltools_0.3.6 Matrix_1.2-17 plyr_1.8.4 ## [22] pkgconfig_2.0.2 broom_0.5.2 haven_2.1.0 ## [25] bookdown_0.11 mvtnorm_1.0-10 scales_1.0.0 ## [28] MatrixModels_0.4-1 htmlTable_1.13.1 mgcv_1.8-28 ## [31] generics_0.0.2 TH.data_1.0-10 withr_2.1.2 ## [34] nnet_7.3-12 lazyeval_0.2.2 cli_1.1.0 ## [37] magrittr_1.5 crayon_1.3.4 readxl_1.3.1 ## [40] polspline_1.1.14 evaluate_0.13 MASS_7.3-51.4 ## [43] nlme_3.1-139 xml2_1.2.0 foreign_0.8-71 ## [46] blogdown_0.13 tools_3.6.0 data.table_1.12.2 ## [49] hms_0.4.2 multcomp_1.4-10 munsell_0.5.0 ## [52] cluster_2.0.8 compiler_3.6.0 rlang_0.4.0 ## [55] grid_3.6.0 rstudioapi_0.10 htmlwidgets_1.3 ## [58] labeling_0.3 base64enc_0.1-3 rmarkdown_1.12 ## [61] codetools_0.2-16 gtable_0.3.0 R6_2.4.0 ## [64] gridExtra_2.3 zoo_1.8-5 lubridate_1.7.4 ## [67] knitr_1.22 stringi_1.4.3 Rcpp_1.0.1 ## [70] rpart_4.1-15 acepack_1.4.1 tidyselect_0.2.5 ## [73] xfun_0.6  ","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567534739,"objectID":"007d3ca1d241508424e31940ef8937c2","permalink":"/post/an-introduction-to-the-harrell-verse-predictive-modeling-using-the-hmisc-and-rms-packages/","publishdate":"2019-09-03T00:00:00Z","relpermalink":"/post/an-introduction-to-the-harrell-verse-predictive-modeling-using-the-hmisc-and-rms-packages/","section":"post","summary":"This is post is to introduce members of the Cincinnati Children’s Hospital Medical Center R Users Group (CCHMC-RUG) to some of the functionality provided by Frank Harrell’s Hmisc and rms packages for data description and predictive modeling. For those of you who have worked with these packages before, hopefully we will cover something new. Dr. Harrell is the founding chair of the Department of Biostatistics at Vanderbilt University (until 2017), Fellow of the American Statistical Association, and a member of the R Foundation.","tags":["R","rms","Hmisc","Data Analysis"],"title":"An Introduction to the Harrell“verse”: Predictive Modeling using the Hmisc and rms Packages","type":"post"},{"authors":[],"categories":["Microbiome"],"content":" \nDuring the Introduction to Metagenomics Summer Workshop we discussed denoising amplicon sequence variants and worked through Ben Callahan’s DADA2 tutorial. During that session, I mentioned several other approaches and algorithms for denoising or clustering amplicon sequence data including UNOISE3, DeBlur and Mothur. I also mentioned I would try to post some example workflows for some of these other approaches to highlight the similarities, as well as the differences. It looks like I am just now getting around to it.\nI was recently involved in a project where we downloaded a large number of amplicon sequence data from the NCBI SRA and denoised these data using UNOISE3. So, I figured this might provide a good opportunity to share some example code on how one could use Robert Edgar’s USEARCH software to process 16S rRNA gene sequence data.\nIn this post, I will not go into much (hardly any) detail about the specifics of each step or the UNOISE3 algorithm itself. This is because Dr. Edgar has an excellent and extensive webpage where he provides all this information already. My goal here is simply to point you to this resource and provide some example code. I highly recommend you spend some time going through his webpage. It provides a wealth of information on many topics and challenges you will encounter when working with metagenomic data. It also provides many of this thoughts on current best practices for processing amplicon sequence data generated via short read technologies. I have been following his work for several years and still pick up something new every time I start digging through his pages.\nOne difference you will notice is that USEARCH does require a license (32-bit is available for free) and is not open-source software. While open-source replications with some modifications do exist, I prefer to use the licensed version as it contains the most up-to-date features and credits the developer’s intellectual contributions. I am also fortunate to be at an institution where this provides no hardship. This choice will be different for everyone.\nSo anyway…lets’ get started.\nThe code below assumes we are working with paired-end sequence files pulled down from the NCBI SRA. The sequence files were generated on the Illumina MiSeq using V4 (515F and 806R) primers. The specific region targeted is not all that relevant to this example; however, we might/should consider different parameters related to the merging of the forward and reverse reads etc. if we had a different amplicon spanning region (i.e. more or less overlap).\nThis workflow also assumes that all non-biological bases and primer sequences have been removed and that all files are stored in a single directory (in this example in a folder on my desktop). I am running 64-bit USEARCH (version 11) for the Mac OS.\n\nRename files to leverage fastq_mergepairs -relabel Paired-end sequence data pulled down from the NCBI SRA has a file structure consisting of the SRR ID followed by \u0026quot;_1\u0026quot; and \u0026quot;_2\u0026quot; denoting the forward and reserve read files, respectively. Providing the data to the fastq_mergepairs command with the suffixes \u0026quot;_R1\u0026quot; and \u0026quot;_R2\u0026quot; allows the -relabel option to identify the corresponding forward and reverse read files (based on the file names) and generate the sample name from the FASTQ filename by truncating at the first underscore or period. This is an extremely helpful option so we will go ahead and rename the files to have the structure expected by the fastq_mergepairs command and -relable option.\n#Renaming files to work with the fastq_mergepairs -relabel command for file in *; do mv \u0026quot;$file\u0026quot; ${file//_1/_R1}; done for file in *; do mv \u0026quot;$file\u0026quot; ${file//_2/_R2}; done \n UNOISE3 pipeline The code below processes the raw fastq files and returns denoised zero-radius OTUs (zOTUs); also known as amplicon sequence variants (ASVs). Robert Edgar provides example scripts for Illumina paired-end (as well as unpaired) data on his website. His scripts served as the basis for the code below…and I have generally tried to maintain his recommendations…with some minor modifications for these data.\nThe code below will:\n Merge the paired-end reads (while performing some filtering) and generate fasta and fastq files containing reads for all samples combined Extract the sample names as a separate text file Filter the combined fastq file based on expected errors derived from the Illumina quality scores Generate a list of unique, high-quality reads allowed to form new seeds (denoised zOTUs) Apply the UNOISE3 algorithm to generate the list of zOTUs Output a denoising report Map the reads for each sample to the list of zOTUs and form an “OTU” table  This section below will take a bit of time to run; however, compared to other denoising approaches USEARCH/UNOISE is very fast and processed these ~2,500 PE files in a few hours. I am allowing for parallel computation to occur where possible using the -threads option (I think 10 cores is actually the upper limit). I also output log files for review at several steps.\nYou could also use an alias for /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 so you do not have to type this each time.\n#Merge paired-end reads /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -fastq_mergepairs *_R1.fastq -relabel @ -fastaout merged.fa -fastqout merged.fq -fastq_maxdiffs 5 -fastq_pctid 90 -fastq_minmergelen 251 -fastq_maxmergelen 257 -log merge_log.txt -threads 16 #Extract sample names /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -fastx_get_sample_names merged.fq -output samples.txt -threads 16 #Filter reads with \u0026gt; 1 expected error /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -fastq_filter merged.fq -fastq_maxee 1.0 -fastaout filtered.fa -relabel Filt -threads 16 -log filter_log.txt #Dereplicate high-quality reads /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -fastx_uniques filtered.fa -sizeout -relabel Uniq -fastaout uniques.fa -threads 16 -log uniques_log.txt #Denoise sequences using UNOISE3 algorithum /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -unoise3 uniques.fa -zotus zotus.fa -threads 16 -log unoise_log.txt #Get denoising report /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -fastx_learn uniques.fa -output denoising_report.txt -threads 16 #Map reads to zOTUs and construct OTU table /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -otutab merged.fq -zotus zotus.fa -strand plus -otutabout unoise3_zotu_table.txt -mapout zmap.txt -threads 16 -log otutab_log.txt \nWhen running the fastq_mergepairs command on this many samples, I got a strange error that I have too many opened folders and it caused the program to error (not process all the files). This looks to be a Mac issue…not USEARCH…and perhaps something I need to tweak/fix on my local machine. While this has only happened to me once, should it happen to you, Robert Edgar kindly recommended a quick and simple workaround. Just place the fastq_mergepairs command in a loop, concatenate the files, and proceed as usual.\nHere is what that code would look like. Just drop it in in place of the first command above.\n#Merge_fastq in a loop to limit the amount of total overhead for read1 in *_R1*; do /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -fastq_mergepairs $read1 -relabel @ -fastaout \u0026quot;$read1\u0026quot;_merged.fa -fastqout \u0026quot;$read1\u0026quot;_merged.fq -fastq_maxdiffs 5 -fastq_pctid 90 -fastq_minmergelen 251 -fastq_maxmergelen 257 -log \u0026quot;$read1\u0026quot;_merge_log.txt -threads 16; done for file in *; do mv \u0026quot;$file\u0026quot; ${file//_R1.fastq_merged/}; done mkdir merge_logs; mv *txt merge_logs/. cat *fa \u0026gt;\u0026gt; merged.fasta cat *fq \u0026gt;\u0026gt; merged.fastq rm *.fa; rm *.fq mv merged.fasta merged.fa; mv merged.fastq merged.fq \n Generate distance matrix and phylogenetic tree Some downstream alpha- and beta-diversity calculations such as Faith’s Phylogenetic Diversity or the UniFrac distance require a phylogenetic tree. In USEARCH you can generate a distance matrix using the calc_distmx command, as well as perform agglomerative clustering and output a Newick formatted tree file using the cluster_aggd command.\nBelow is how one would do this.\n#Generate distance matrix /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -calc_distmx zotus.fa -tabbedout zotus_dm.txt -threads 16 -log calcdist_log.txt #Perform agglomerative clustering and output tree file /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -cluster_aggd zotus_dm.txt -treeout unoise3.tree -threads 16 -log cluster_tree_log.txt \n Taxonomic classification with SINTAX Taxonomic classification can be performed within USEARCH using the sintax command. The sintax command uses the SINTAX algorithm to predict taxonomy for query sequences in FASTA or FASTQ format. Here is the link to a description of the approach on Dr. Edgar’s webpage. Here are links to the SINTAX publication and a link to a discussion on which database should be used for classifaction. In short, he recommends using a database of authoritatively classified sequences such as the most recent RDP training set or LTP release that includes only type strain and isolate sequences; and not predictions.\nI like the SINTAX algorithm as it performs as well as the naive RDP classifier (not sure many algorithums do much better) and the approach is more intuitive to me and just requires finding the top hits in a reference database.\nHere is the code to classify these sequences against the RDP database at a cutoff of 0.8. Links to this database and others are provided here.\n#Obtain taxonomy predictions with SINTAX /Users/olljt2/documents/usearch/v11/usearch11.0.667_i86osx64 -sintax zotus.fa -db /Users/olljt2/documents/usearch/v11/rdp_16s_v16.fa -strand both -tabbedout rdp_sintax_unoise3.txt -sintax_cutoff 0.8 -threads 16 -log sintax_log.txt \n Combine elements into a phyloseq object While Robert Edgar has built some nice new functionally to perform downstream analyses using USEARCH directly, I typically conduct of most of my statistical analysis in R. This has nothing to do with the available commands in USEARCH, rather the choice is based on my familiarity with R and the large number of packages that are now available, and continuing to grow, for the downstream analysis of metagenomic data.\nWhen I work with metagenomic data in R, my first step is always to store the data as a phyloseq object to facilitate improved data management and analysis options. The code below can be run within R to:\n Read in the representative sequences, phylogenic tree, OTU table, and taxonomy predictions generated above Parse the taxonomy table Create a phyloseq object Add the sample name as a sample_data column  I typically just save this as an R script and call it form the command line with: Rscript /Users/olljt2/Desktop/unoise3_to_phyloseq.R\nThe parsing of the taxonomy table is specific to the structure of RDP database. This would need to be modified to be used with a different database such as Silva or Greengenes.\n#!/usr/bin/Rscript library(tidyverse); packageVersion(\u0026quot;tidyverse\u0026quot;) library(phyloseq); packageVersion(\u0026quot;phyloseq\u0026quot;) rep = Biostrings::readDNAStringSet(\u0026quot;zotus.fa\u0026quot;) TREE \u0026lt;- read_tree(\u0026quot;unoise3.tree\u0026quot;) seq.tab \u0026lt;- read.delim(\u0026quot;unoise3_zotu_table.txt\u0026quot;) seq.tab \u0026lt;- data.frame(seq.tab[,-1], row.names=seq.tab[, 1]) tax.table \u0026lt;- read.delim(\u0026quot;rdp_sintax_unoise3.txt\u0026quot;, header = FALSE) tax_table \u0026lt;- tax.table %\u0026gt;% select(V1, V4) %\u0026gt;% separate(V4, c(\u0026quot;Domain\u0026quot;, \u0026quot;Phylum\u0026quot;, \u0026quot;Class\u0026quot;, \u0026quot;Order\u0026quot;, \u0026quot;Family\u0026quot;, \u0026quot;Genus\u0026quot;, \u0026quot;Species\u0026quot;), sep = \u0026quot;,\u0026quot;) %\u0026gt;% mutate(Domain = gsub(\u0026quot;d:\u0026quot;, \u0026quot;\u0026quot;, Domain), Phylum = gsub(\u0026quot;p:\u0026quot;, \u0026quot;\u0026quot;, Phylum), Class = gsub(\u0026quot;c:\u0026quot;, \u0026quot;\u0026quot;, Class), Order = gsub(\u0026quot;o:\u0026quot;, \u0026quot;\u0026quot;, Order), Family = gsub(\u0026quot;f:\u0026quot;, \u0026quot;\u0026quot;, Family), Genus = gsub(\u0026quot;g:\u0026quot;, \u0026quot;\u0026quot;, Genus), Species = gsub(\u0026quot;s:\u0026quot;, \u0026quot;\u0026quot;, Species)) tax_table \u0026lt;- data.frame(tax_table[,-1], row.names=tax_table[, 1]) (ps_unoise3 \u0026lt;- phyloseq(otu_table(seq.tab, taxa_are_rows=TRUE), tax_table(as.matrix(tax_table)), rep, TREE)) samp_df \u0026lt;- data.frame(sample_names(ps_unoise3)) samp_df$srr_id \u0026lt;- samp_df$sample_names.ps_unoise3. samp_df \u0026lt;- data.frame(samp_df, row.names = 1) (ps_unoise3 \u0026lt;- merge_phyloseq(ps_unoise3, sample_data(samp_df))) saveRDS(ps_unoise3, \u0026quot;ps_unoise3.rds\u0026quot;) \n Clean up file structure I think the goal of this section is self-evident. This code just tidies up all the files generated above. Add a line to compress the files if you would like.\n#Cleaning up file structure mkdir logs; mkdir seqs; mkdir unoise mv *unoise3* unoise/.; mv zmap.txt unoise/.; mv zotus.fa unoise/.; mv zotus_dm.txt unoise/. mv denoising_report.txt logs/.; mv *log.txt logs/. ; mv samples.txt logs/. mv *.fa seqs/.; mv *.fq seqs/.; mv *.fastq seqs/. \n Bonus: Working with single-end reads On a few occasions I have been involved with projects that only conducted single-end sequencing (rare) or older data and/or poor choice of primers where only the forward reads could be retained for analysis (less rare). The modification to the above workflow for single-end data is quite simple.\nThe only material difference to the UNOISE pipeline describe above is to remove the merge paired-end reads step and just replace it by concatenating all the single-end read files into a combined fastq file. Then you can proceed with the rest of the steps as provided above.\nThe code to combine all the fastq files, assuming they are all placed in the same directory (and end in fastq) and the directory does not contain any other fastq files, just requires the cat command and outputting the new file. It’s always nice when things are simple!\ncat *fastq \u0026gt;\u0026gt; raw_merged.fq \nI hope the Explanations and code snippets above are helpful. Please send me any questions or post them here.\nA great way to get started using USEARCH would be to download a 32-bit version of USEARCH and work through Dr. Edgar’s example scripts and tutorials.\nHowever, you may require a 64-bit license to work with your own data.\n ","date":1566345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566402070,"objectID":"f77e8796d80ab5d8804a9c89e369601d","permalink":"/post/denoising-amplicon-sequence-data-using-usearch-and-unoise3/","publishdate":"2019-08-21T00:00:00Z","relpermalink":"/post/denoising-amplicon-sequence-data-using-usearch-and-unoise3/","section":"post","summary":"During the Introduction to Metagenomics Summer Workshop we discussed denoising amplicon sequence variants and worked through Ben Callahan’s DADA2 tutorial. During that session, I mentioned several other approaches and algorithms for denoising or clustering amplicon sequence data including UNOISE3, DeBlur and Mothur. I also mentioned I would try to post some example workflows for some of these other approaches to highlight the similarities, as well as the differences. It looks like I am just now getting around to it.","tags":["Microbiome","R","Phyloseq","SRA","USEARCH","UNOISE3"],"title":"Denoising Amplicon Sequence Data Using USEARCH and UNOISE3","type":"post"},{"authors":[],"categories":["Microbiome"],"content":" \nA collaborator recently asked if I could help pull down a few thousand sequence files from the NCBI Sequence Read Archive (SRA) for a secondary analysis. This is a short post primarily to help me (and hopefully others) remember how to do this once you have a set of SRR IDs of interest.\nWhile I came across several great resources providing information on how to download SRA files using the SRA Toolkit, I wanted to retain just the basics, and some example code, should this type of request come across my desk again in the future. Hopefully this post will keep me from having to start from scratch the next time this comes up and not rehash the same mistakes I made the first time around.\nThere are several great resources for learning more about accessing data and metadata from the SRA including:\n The NCBI SRA Download Guide An excellent series of posts from Rob Edwards Morgan Langille’s very helpful Download_From_SRA wiki  \nHowever, for this type of request, all I think I will need to remember is a few lines of code and that I want to grab the fastq files, in parallel and without compression, to save time.\nSo below is what I think will be most useful once one has a set of SRR IDs of interest.\n\nInstalling the SRA ToolKit using Homebrew: I typically work on an iMac pro and have access to multiple cores for parallel computing. Installation of the SRA Toolkit can be performed quickly on a Mac using Homebrew. Installing parallel will also allow you to perform the download…well…in parallel.\nIf you are working on a Linux or Windows machine binaries can be found here.\nThe code below will install both the toolkit and parallel.\n#Install SRA toolkit brew install sratoolkit #Install parallel brew install parallel \n Create a new folder that will contain the files: Now we simply want to open the terminal, create a new folder that will house the sequence files, and navigate to that folder before running the fastq-dump command. Here I am creating a new folder on my desktop that will store a bunch of fastq files generated from 16s rRNA gene sequencing on the Illumina MiSeq. As you can guess from the name, these files come from the well-known RISK IBD cohort.\n#Make new folder that will contain the seqs mkdir /Users/olljt2/desktop/risk_16s_seqs #Move into the folder before starting download cd /Users/olljt2/desktop/risk_16s_seqs \n Download the SRR runs: Now the fun part. The fastq-dump command will:\n Download the sequence data for each SRR ID contained in the /Users/olljt2/desktop/sra_ids.txt text file Download the data as fastq files (without compression) Run the job over 16 threads (modify per the number available on your machine) Dump each run into a separate file (so we will have either 1 or 2 fastqs per SRR ID depending on whether the run used single- or paired-end sequencing)  A full list of the fastq-dump options can be found here. I prefer to download the files without compression as this greatly reduces the download time. Once the files are on your local machine, you can then go ahead and compress them at your leisure without worrying about maintaining the connection.\nparallel --jobs 16 \u0026quot;fastq-dump --split-files {}\u0026quot; ::: $(cat /Users/olljt2/desktop/sra_ids.txt) \nThe sra_ids.txt text file called above is simply a tab-delimited text file with no header that has a structure of…\nSRR2061872 SRR1636304 SRR1635772 SRR1213188 SRR1565299 \nHopefully, I will now remember where to find this information, and snippets of code, the next time the need arises!\n\n ","date":1566259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566401890,"objectID":"b8cb60ab8265aeecfd9f15f60b99dabb","permalink":"/post/downloading-amplicon-sequence-runs-from-the-ncbi-sra/","publishdate":"2019-08-20T00:00:00Z","relpermalink":"/post/downloading-amplicon-sequence-runs-from-the-ncbi-sra/","section":"post","summary":"A collaborator recently asked if I could help pull down a few thousand sequence files from the NCBI Sequence Read Archive (SRA) for a secondary analysis. This is a short post primarily to help me (and hopefully others) remember how to do this once you have a set of SRR IDs of interest.\nWhile I came across several great resources providing information on how to download SRA files using the SRA Toolkit, I wanted to retain just the basics, and some example code, should this type of request come across my desk again in the future.","tags":["Microbiome","SRA"],"title":"Downloading Amplicon Sequence Runs from the NCBI SRA","type":"post"},{"authors":[],"categories":["Microbiome"],"content":"  \nThis post is from a tutorial demonstrating the processing of amplicon short read data in R taught as part of the Introduction to Metagenomics Summer Workshop. It provides a quick introduction some of the functionality provided by phyloseq and follows some of Paul McMurdie’s excellent tutorials. This tutorial picks up where Ben Callahan’s DADA2 tutorial leaves off and highlights some of the main accessor and processor functions of the package. I thought it might be useful to a broader audience so decided to post it.\n\nThe goal of this interactive session is to introduce you to some of the basic functionality of the phyloseq package that can help you to explore and better understand your metagenomic data. We will be working with the phyloseq object that was created during the DADA2 tutorial. If you recall, these were murine stool samples collected from a single mouse over time. The phyloseq object contains: an ASV table, sample metadata, taxonomic classifications, and the reference sequences. We did not generate a phylogenetic tree from these sequences, but if we had, it could be included as well.\nThe session will quickly cover some of the basic accessor, analysis and graphical functions available to you when using the phyloseq package in R.\nTo learn more, Paul McMurdie has an excellent set of tutorials that I encourage you to explore.\n https://joey711.github.io/phyloseq/preprocess.html https://joey711.github.io/phyloseq/index.html  \n Loading required packages and phyloseq object library(dada2); packageVersion(\u0026quot;dada2\u0026quot;)  ## Loading required package: Rcpp ## [1] \u0026#39;1.12.1\u0026#39; library(phyloseq); packageVersion(\u0026quot;phyloseq\u0026quot;)  ## [1] \u0026#39;1.28.0\u0026#39; library(ggplot2); packageVersion(\u0026quot;ggplot2\u0026quot;)  ## [1] \u0026#39;3.2.0\u0026#39; \nIf the phyloseq (ps) object is not already loaded into your environment…let’s go ahead and do that now. You will need to change the path so that it maps to the ps object on your computer. No path is needed if you are working in an RStudio project folder (or if you cloned the folder from GitHub).\nps \u0026lt;- readRDS(\u0026quot;C:/Users/olljt2/Desktop/academic_web_page/static/data/ps.rds\u0026quot;) \n Accessing the sample information and sample metadata ps ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 232 taxa and 19 samples ] ## sample_data() Sample Data: [ 19 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 232 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 232 reference sequences ]  Here we can see that we have a phyloseq object that consists of:  An OTU table with 232 taxa and 19 samples A sample metadata file consisting of 4 variables A taxonomy table with 7 ranks Reference sequences on all 232 taxa   \nThis highlights one of the key advantages of working with phyloseq objects in R. Each of these data structures is contained in a single object. This makes it easy to keep all of your data together and to share it with colleagues or include it as a supplemental file to a publication.\nNext we will see how each of the components can be accessed. We will run through several commands below and then discuss the output.\nnsamples(ps) ## [1] 19 sample_names(ps) ## [1] \u0026quot;F3D0\u0026quot; \u0026quot;F3D1\u0026quot; \u0026quot;F3D141\u0026quot; \u0026quot;F3D142\u0026quot; \u0026quot;F3D143\u0026quot; \u0026quot;F3D144\u0026quot; \u0026quot;F3D145\u0026quot; ## [8] \u0026quot;F3D146\u0026quot; \u0026quot;F3D147\u0026quot; \u0026quot;F3D148\u0026quot; \u0026quot;F3D149\u0026quot; \u0026quot;F3D150\u0026quot; \u0026quot;F3D2\u0026quot; \u0026quot;F3D3\u0026quot; ## [15] \u0026quot;F3D5\u0026quot; \u0026quot;F3D6\u0026quot; \u0026quot;F3D7\u0026quot; \u0026quot;F3D8\u0026quot; \u0026quot;F3D9\u0026quot; sample_variables(ps) ## [1] \u0026quot;Subject\u0026quot; \u0026quot;Gender\u0026quot; \u0026quot;Day\u0026quot; \u0026quot;When\u0026quot; head(sample_data(ps)) ## Subject Gender Day When ## F3D0 3 F 0 Early ## F3D1 3 F 1 Early ## F3D141 3 F 141 Late ## F3D142 3 F 142 Late ## F3D143 3 F 143 Late ## F3D144 3 F 144 Late sample_data(ps)$When ## [1] \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; ## [9] \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Late\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; ## [17] \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; table(sample_data(ps)$When) ## ## Early Late ## 9 10 median(sample_data(ps)$Day) ## [1] 141 metadata \u0026lt;- data.frame(sample_data(ps)) head(metadata) ## Subject Gender Day When ## F3D0 3 F 0 Early ## F3D1 3 F 1 Early ## F3D141 3 F 141 Late ## F3D142 3 F 142 Late ## F3D143 3 F 143 Late ## F3D144 3 F 144 Late Here we can see that we have 19 samples and they are assigned the sample names we gave them during the DADA2 tutorial. We also have 4 variables (Subject, Gender, Day, and When) and that information can be easily accessed and computations or descriptive statistics performed. Specific components of the ps object can be extracted and converted to a data.frame for additional analyses.\n\n Examining the number of reads for each sample Phyloseq makes it easy to calculate the total number of reads for each sample, sort them to identify potentially problematic samples, and plot their distribution.\nsample_sums(ps) ## F3D0 F3D1 F3D141 F3D142 F3D143 F3D144 F3D145 F3D146 F3D147 F3D148 ## 6528 5017 4863 2521 2518 3488 5820 3879 13006 9935 ## F3D149 F3D150 F3D2 F3D3 F3D5 F3D6 F3D7 F3D8 F3D9 ## 10653 4240 16835 5491 3716 6679 4217 4547 6015 sort(sample_sums(ps)) ## F3D143 F3D142 F3D144 F3D5 F3D146 F3D7 F3D150 F3D8 F3D141 F3D1 ## 2518 2521 3488 3716 3879 4217 4240 4547 4863 5017 ## F3D3 F3D145 F3D9 F3D0 F3D6 F3D148 F3D149 F3D147 F3D2 ## 5491 5820 6015 6528 6679 9935 10653 13006 16835 hist(sample_sums(ps), main=\u0026quot;Histogram: Read Counts\u0026quot;, xlab=\u0026quot;Total Reads\u0026quot;, border=\u0026quot;blue\u0026quot;, col=\u0026quot;green\u0026quot;, las=1, breaks=12) metadata$total_reads \u0026lt;- sample_sums(ps) Here we see that the number of reads per sample ranges from 2,518 to 16,835 and most samples have less than 10k reads. Try to calculate the mean and median number of reads on your own.\nThe last line of code above can be used to add a new column containing the total read count to the metadata data.frame. Similarly, sample_data(ps)$total_reads \u0026lt;- sample_sums(ps) would add this information to the phyloseq object itself (as a new sample_data variable).\n\n Examining the OTU table ntaxa(ps) ## [1] 232 head(taxa_names(ps)) ## [1] \u0026quot;ASV1\u0026quot; \u0026quot;ASV2\u0026quot; \u0026quot;ASV3\u0026quot; \u0026quot;ASV4\u0026quot; \u0026quot;ASV5\u0026quot; \u0026quot;ASV6\u0026quot; head(taxa_sums(ps)) ## ASV1 ASV2 ASV3 ASV4 ASV5 ASV6 ## 14148 9898 8862 7935 5880 5469 (asv_tab \u0026lt;- data.frame(otu_table(ps)[1:5, 1:5])) ## ASV1 ASV2 ASV3 ASV4 ASV5 ## F3D0 579 345 449 430 154 ## F3D1 405 353 231 69 140 ## F3D141 444 362 345 502 189 ## F3D142 289 304 158 164 180 ## F3D143 228 176 204 231 130  Phyloseq allows you to easily:  Obtain a count of the number of taxa Access their names (e.g. ASV1, ASV2, …) Get a count of each ASV summed over all samples Extract the OTU table as a data.frame   \n Examining the taxonomy rank_names(ps) ## [1] \u0026quot;Kingdom\u0026quot; \u0026quot;Phylum\u0026quot; \u0026quot;Class\u0026quot; \u0026quot;Order\u0026quot; \u0026quot;Family\u0026quot; \u0026quot;Genus\u0026quot; \u0026quot;Species\u0026quot; head(tax_table(ps)) ## Taxonomy Table: [6 taxa by 7 taxonomic ranks]: ## Kingdom Phylum Class Order ## ASV1 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV2 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV3 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV4 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV5 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV6 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## Family Genus Species ## ASV1 \u0026quot;Muribaculaceae\u0026quot; NA NA ## ASV2 \u0026quot;Muribaculaceae\u0026quot; NA NA ## ASV3 \u0026quot;Muribaculaceae\u0026quot; NA NA ## ASV4 \u0026quot;Muribaculaceae\u0026quot; NA NA ## ASV5 \u0026quot;Bacteroidaceae\u0026quot; \u0026quot;Bacteroides\u0026quot; NA ## ASV6 \u0026quot;Muribaculaceae\u0026quot; NA NA head(tax_table(ps)[, 2]) ## Taxonomy Table: [6 taxa by 1 taxonomic ranks]: ## Phylum ## ASV1 \u0026quot;Bacteroidetes\u0026quot; ## ASV2 \u0026quot;Bacteroidetes\u0026quot; ## ASV3 \u0026quot;Bacteroidetes\u0026quot; ## ASV4 \u0026quot;Bacteroidetes\u0026quot; ## ASV5 \u0026quot;Bacteroidetes\u0026quot; ## ASV6 \u0026quot;Bacteroidetes\u0026quot; table(tax_table(ps)[, 2]) ## ## Actinobacteria Bacteroidetes Cyanobacteria ## 6 20 3 ## Deinococcus-Thermus Epsilonbacteraeota Firmicutes ## 1 1 185 ## Patescibacteria Proteobacteria Tenericutes ## 2 7 6 ## Verrucomicrobia ## 1 (tax_tab \u0026lt;- data.frame(tax_table(ps)[50:55, ])) ## Kingdom Phylum Class Order ## ASV50 Bacteria Firmicutes Clostridia Clostridiales ## ASV51 Bacteria Firmicutes Clostridia Clostridiales ## ASV52 Bacteria Firmicutes Clostridia Clostridiales ## ASV53 Bacteria Epsilonbacteraeota Campylobacteria Campylobacterales ## ASV54 Bacteria Firmicutes Clostridia Clostridiales ## ASV55 Bacteria Firmicutes Clostridia Clostridiales ## Family Genus Species ## ASV50 Lachnospiraceae Acetatifactor \u0026lt;NA\u0026gt; ## ASV51 Ruminococcaceae Ruminiclostridium_5 \u0026lt;NA\u0026gt; ## ASV52 Lachnospiraceae Lachnospiraceae_UCG-001 \u0026lt;NA\u0026gt; ## ASV53 Helicobacteraceae Helicobacter pylori ## ASV54 Family_XIII \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## ASV55 Ruminococcaceae Ruminiclostridium_5 \u0026lt;NA\u0026gt; Here we can see that we have information on 7 taxonomic ranks from Kingdom to Species. We can easily access specific components of this object to learn more about the classifications. For example, we see that the vast majority of ASVs are classified as Firmicutes. This is in line with our expectations. Conducting such assessments may help you identify potential sequencing errors that made it through the denoising pipeline (i.e. those not assigned to a Kingdom) or to understand the proportion of sequences classified at lower levels (i.e. genus or species).\nOne could also swap out this taxonomy file for another…say using the IDTAXA function in the DECIPHER package or an alternative reference database (i.e. Silva or Greengenes). I will let you look into this on your own!\n\n Examining the reference sequences Storing the reference sequences with your phyloseq object is critical of you rename the ASV names to ASV1, ASV2, … This will allow you to communicate the information on these ASVs directly (i.e. you can provide the exact sequence variant information). This information is also required to build a phylogenetic tree or BLAST the sequences against the NCBI database for example. In short, always include these in the phyloseq object.\nBelow we see that these sequences are stored as a DNAStringSet. The refseq command returns the ASV number, sequence length, and amplicon sequence for each ASV. The function dada2::nwhamming is calculating the Hamming distance of two sequences after alignment. We will discuss more about this in class. We can also pull out the component and store it as a data.frame.\nhead(refseq(ps)) ## A DNAStringSet instance of length 6 ## width seq names ## [1] 252 TACGGAGGATGCGAGCGTTAT...AAGTGTGGGTATCGAACAGG ASV1 ## [2] 252 TACGGAGGATGCGAGCGTTAT...AAGCGTGGGTATCGAACAGG ASV2 ## [3] 252 TACGGAGGATGCGAGCGTTAT...AAGCGTGGGTATCGAACAGG ASV3 ## [4] 252 TACGGAGGATGCGAGCGTTAT...AAGTGCGGGGATCGAACAGG ASV4 ## [5] 253 TACGGAGGATCCGAGCGTTAT...AAGTGTGGGTATCAAACAGG ASV5 ## [6] 252 TACGGAGGATGCGAGCGTTAT...AAGTGCGGGGATCAAACAGG ASV6 dada2::nwhamming(as.vector(refseq(ps)[1]), as.vector(refseq(ps)[2])) ## [1] 20 (ref_tab \u0026lt;- data.frame(head(refseq(ps)))) ## head.refseq.ps.. ## ASV1 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGCAGGCGGAAGATCAAGTCAGCGGTAAAATTGAGAGGCTCAACCTCTTCGAGCCGTTGAAACTGGTTTTCTTGAGTGAGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCATACCGGCGCTCAACTGACGCTCATGCACGAAAGTGTGGGTATCGAACAGG ## ASV2 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGCAGGCGGACTCTCAAGTCAGCGGTCAAATCGCGGGGCTCAACCCCGTTCCGCCGTTGAAACTGGGAGCCTTGAGTGCGCGAGAAGTAGGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCCTACCGGCGCGCAACTGACGCTCATGCACGAAAGCGTGGGTATCGAACAGG ## ASV3 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGGCTGTTAAGTCAGCGGTCAAATGTCGGGGCTCAACCCCGGCCTGCCGTTGAAACTGGCGGCCTCGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACTCCGATTGCGAAGGCAGCATACCGGCGCCCGACTGACGCTGAGGCACGAAAGCGTGGGTATCGAACAGG ## ASV4 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGGCTTTTAAGTCAGCGGTAAAAATTCGGGGCTCAACCCCGTCCGGCCGTTGAAACTGGGGGCCTTGAGTGGGCGAGAAGAAGGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACCCCGATTGCGAAGGCAGCCTTCCGGCGCCCTACTGACGCTGAGGCACGAAAGTGCGGGGATCGAACAGG ## ASV5 TACGGAGGATCCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGAGCGTAGGTGGATTGTTAAGTCAGTTGTGAAAGTTTGCGGCTCAACCGTAAAATTGCAGTTGAAACTGGCAGTCTTGAGTACAGTAGAGGTGGGCGGAATTCGTGGTGTAGCGGTGAAATGCTTAGATATCACGAAGAACTCCGATTGCGAAGGCAGCTCACTGGACTGCAACTGACACTGATGCTCGAAAGTGTGGGTATCAAACAGG ## ASV6 TACGGAGGATGCGAGCGTTATCCGGATTTATTGGGTTTAAAGGGTGCGTAGGCGGCCTGCCAAGTCAGCGGTAAAATTGCGGGGCTCAACCCCGTACAGCCGTTGAAACTGCCGGGCTCGAGTGGGCGAGAAGTATGCGGAATGCGTGGTGTAGCGGTGAAATGCATAGATATCACGCAGAACCCCGATTGCGAAGGCAGCATACCGGCGCCCTACTGACGCTGAGGCACGAAAGTGCGGGGATCAAACAGG \n Accessing the phylogenetic tree We did not generate a phylogenetic tree during the DADA2 tutorial in the interest of time. However, phyloseq has many excellent tools for working with and visualizing trees. I recommend you take a look at these tutorials below for some examples.\n https://joey711.github.io/phyloseq/preprocess.html https://joey711.github.io/phyloseq/plot_tree-examples.html  Ben Callahan’s F1000 paper demonstrates a complete analysis workflow in R including the construction of a de-novo phylogenetic tree. I highly recommned you take a look at this paper.\n\n Agglomerating and subsetting taxa Often times we may want to agglomerate taxa to a specific taxonomic rank for analysis. Or we may want to work with a given subset of taxa. We can perform these operations in phyloseq with the tax_glom and subset_taxa functions.\n(ps_phylum \u0026lt;- tax_glom(ps, \u0026quot;Phylum\u0026quot;)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 10 taxa and 19 samples ] ## sample_data() Sample Data: [ 19 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 10 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 10 reference sequences ] taxa_names(ps_phylum) ## [1] \u0026quot;ASV1\u0026quot; \u0026quot;ASV11\u0026quot; \u0026quot;ASV19\u0026quot; \u0026quot;ASV53\u0026quot; \u0026quot;ASV67\u0026quot; \u0026quot;ASV90\u0026quot; \u0026quot;ASV107\u0026quot; ## [8] \u0026quot;ASV109\u0026quot; \u0026quot;ASV189\u0026quot; \u0026quot;ASV191\u0026quot; taxa_names(ps_phylum) \u0026lt;- tax_table(ps_phylum)[, 2] taxa_names(ps_phylum) ## [1] \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Firmicutes\u0026quot; \u0026quot;Tenericutes\u0026quot; ## [4] \u0026quot;Epsilonbacteraeota\u0026quot; \u0026quot;Actinobacteria\u0026quot; \u0026quot;Patescibacteria\u0026quot; ## [7] \u0026quot;Proteobacteria\u0026quot; \u0026quot;Deinococcus-Thermus\u0026quot; \u0026quot;Cyanobacteria\u0026quot; ## [10] \u0026quot;Verrucomicrobia\u0026quot; otu_table(ps_phylum)[1:5, c(1:3, 5, 7)] ## OTU Table: [5 taxa and 5 samples] ## taxa are columns ## Bacteroidetes Firmicutes Tenericutes Actinobacteria Proteobacteria ## F3D0 3708 2620 151 27 12 ## F3D1 1799 3011 157 3 16 ## F3D141 3437 1370 35 16 0 ## F3D142 2003 452 33 28 0 ## F3D143 1816 655 34 10 0 Here we are agglomerating the counts to the Phylum-level and then renaming the ASVs to make them more descriptive. We can see that we have 10 Phyla. The ASV information (i.e. refseq and taxonomy for one of the ASVs in each Phylum) gets carried along for the ride (we can typically ignore this or you can remove these components if you prefer).\n\nWe can also subset taxa… (ps_bacteroides \u0026lt;- subset_taxa(ps, Genus == \u0026quot;Bacteroides\u0026quot;)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 3 taxa and 19 samples ] ## sample_data() Sample Data: [ 19 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 3 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 3 reference sequences ] tax_table(ps_bacteroides) ## Taxonomy Table: [3 taxa by 7 taxonomic ranks]: ## Kingdom Phylum Class Order ## ASV5 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV80 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## ASV163 \u0026quot;Bacteria\u0026quot; \u0026quot;Bacteroidetes\u0026quot; \u0026quot;Bacteroidia\u0026quot; \u0026quot;Bacteroidales\u0026quot; ## Family Genus Species ## ASV5 \u0026quot;Bacteroidaceae\u0026quot; \u0026quot;Bacteroides\u0026quot; NA ## ASV80 \u0026quot;Bacteroidaceae\u0026quot; \u0026quot;Bacteroides\u0026quot; \u0026quot;vulgatus\u0026quot; ## ASV163 \u0026quot;Bacteroidaceae\u0026quot; \u0026quot;Bacteroides\u0026quot; \u0026quot;vulgatus\u0026quot; prune_taxa(taxa_sums(ps) \u0026gt; 100, ps)  ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 99 taxa and 19 samples ] ## sample_data() Sample Data: [ 19 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 99 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 99 reference sequences ] filter_taxa(ps, function(x) sum(x \u0026gt; 10) \u0026gt; (0.1*length(x)), TRUE)  ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 135 taxa and 19 samples ] ## sample_data() Sample Data: [ 19 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 135 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 135 reference sequences ]  With the above commands we can quickly see that we have:  A total of 3 ASVs classified as Bacteroides A total of 99 ASVs seen at least 100 times across all samples A total of 135 taxa seen at least 10 times in at least 10% of samples   This highlights how we might use phyloseq as a tool to filter taxa prior to statistical analysis.\n\n  Subsetting samples and tranforming counts Phyloseq can also be used to subset all the individual components based on sample metadata information. This would take a fair bit of work to do properly if we were working with each individual component…and not with phyloseq. Below we subset the early stool samples. Then we generate an object that includes only samples with \u0026gt; 5,000 total reads.\nps_early \u0026lt;- subset_samples(ps, When == \u0026quot;Early\u0026quot;) (ps_early = prune_taxa(taxa_sums(ps_early) \u0026gt; 0, ps_early)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 183 taxa and 9 samples ] ## sample_data() Sample Data: [ 9 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 183 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 183 reference sequences ] sample_data(ps_early)$When ## [1] \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; \u0026quot;Early\u0026quot; sort(sample_sums(ps)) ## F3D143 F3D142 F3D144 F3D5 F3D146 F3D7 F3D150 F3D8 F3D141 F3D1 ## 2518 2521 3488 3716 3879 4217 4240 4547 4863 5017 ## F3D3 F3D145 F3D9 F3D0 F3D6 F3D148 F3D149 F3D147 F3D2 ## 5491 5820 6015 6528 6679 9935 10653 13006 16835 (ps_reads_GT_5k = prune_samples(sample_sums(ps) \u0026gt; 5000, ps)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 232 taxa and 10 samples ] ## sample_data() Sample Data: [ 10 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 232 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 232 reference sequences ] sort(sample_sums(ps_reads_GT_5k)) ## F3D1 F3D3 F3D145 F3D9 F3D0 F3D6 F3D148 F3D149 F3D147 F3D2 ## 5017 5491 5820 6015 6528 6679 9935 10653 13006 16835 \nCounts can be converted to relative abundances (e.g. total sum scaling) using the transform_sample_counts function. They can also be subsampled/rarified using the rarefy_even_depth function. However, subsampling to account for differences in sequencing depth acorss samples has important limitations. See the papers below for a more in-depth discussion.\n McMurdie and Holmes, Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible Weiss et. al., Normalization and microbial differential abundance strategies depend upon data characteristics  \nps_relabund \u0026lt;- transform_sample_counts(ps, function(x) x / sum(x)) otu_table(ps_relabund)[1:5, 1:5] ## OTU Table: [5 taxa and 5 samples] ## taxa are columns ## ASV1 ASV2 ASV3 ASV4 ASV5 ## F3D0 0.08869485 0.05284926 0.06878064 0.06587010 0.02359069 ## F3D1 0.08072553 0.07036077 0.04604345 0.01375324 0.02790512 ## F3D141 0.09130167 0.07443965 0.07094386 0.10322846 0.03886490 ## F3D142 0.11463705 0.12058707 0.06267354 0.06505355 0.07140024 ## F3D143 0.09054805 0.06989674 0.08101668 0.09173948 0.05162828 (ps_rare \u0026lt;- rarefy_even_depth(ps, sample.size = 4000, rngseed = 123, replace = FALSE)) ## `set.seed(123)` was used to initialize repeatable random subsampling. ## Please record this for your records so others can reproduce. ## Try `set.seed(123); .Random.seed` for the full vector ## ... ## 5 samples removedbecause they contained fewer reads than `sample.size`. ## Up to first five removed samples are: ## F3D142F3D143F3D144F3D146F3D5 ## ... ## 15OTUs were removed because they are no longer ## present in any sample after random subsampling ## ... ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 217 taxa and 14 samples ] ## sample_data() Sample Data: [ 14 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 217 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 217 reference sequences ] sample_sums(ps_rare) ## F3D0 F3D1 F3D141 F3D145 F3D147 F3D148 F3D149 F3D150 F3D2 F3D3 ## 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 ## F3D6 F3D7 F3D8 F3D9 ## 4000 4000 4000 4000 \n Example analytic and graphical capabilities Phyloseq has an extensive list of functions for processing and analyzing microbiome data. I recommend you view the tutorial section on the phyloseq home page to get a feel for all that phyloseq can do. Below are just a few quick examples. We will get more into these types of analyses in subsequent sessions.\nAlpha-diversity Below we will receive a warning that our data does not contain any singletons and that the results of richness estimates are probably unreliable. This is an important point and we will delve into this issue more in the next session. For now, you can go ahead and ignore the warning.\nhead(estimate_richness(ps)) ## Warning in estimate_richness(ps): The data you have provided does not have ## any singletons. This is highly suspicious. Results of richness ## estimates (for example) are probably unreliable, or wrong, if you have already ## trimmed low-abundance taxa from the data. ## ## We recommended that you find the un-trimmed data and retry. ## Observed Chao1 se.chao1 ACE se.ACE Shannon Simpson InvSimpson ## F3D0 106 106 0 106 4.539138 3.865881 0.9644889 28.16024 ## F3D1 100 100 0 100 4.208325 3.993196 0.9709838 34.46347 ## F3D141 74 74 0 74 3.878214 3.428895 0.9501123 20.04502 ## F3D142 48 48 0 48 3.388092 3.117940 0.9386949 16.31185 ## F3D143 56 56 0 56 3.543102 3.292717 0.9464422 18.67141 ## F3D144 47 47 0 47 3.135249 2.994201 0.9309895 14.49054 ## Fisher ## F3D0 17.973004 ## F3D1 17.696857 ## F3D141 12.383762 ## F3D142 8.412094 ## F3D143 10.148818 ## F3D144 7.678694 (p \u0026lt;- plot_richness(ps, x = \u0026quot;When\u0026quot;, color = \u0026quot;When\u0026quot;, measures = c(\u0026quot;Observed\u0026quot;, \u0026quot;Shannon\u0026quot;))) ## Warning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have ## any singletons. This is highly suspicious. Results of richness ## estimates (for example) are probably unreliable, or wrong, if you have already ## trimmed low-abundance taxa from the data. ## ## We recommended that you find the un-trimmed data and retry. p + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Alpha Diversity Measure\\n\u0026quot;) + theme_bw()  Beta-diversity ordination ps_rare_bray \u0026lt;- ordinate(ps_rare, \u0026quot;NMDS\u0026quot;, \u0026quot;bray\u0026quot;) ## Square root transformation ## Wisconsin double standardization ## Run 0 stress 0.08484704 ## Run 1 stress 0.08484704 ## ... New best solution ## ... Procrustes: rmse 2.497137e-06 max resid 5.691675e-06 ## ... Similar to previous best ## Run 2 stress 0.09657264 ## Run 3 stress 0.08484704 ## ... Procrustes: rmse 7.186183e-07 max resid 1.423558e-06 ## ... Similar to previous best ## Run 4 stress 0.08484704 ## ... Procrustes: rmse 3.303025e-06 max resid 7.565974e-06 ## ... Similar to previous best ## Run 5 stress 0.1744901 ## Run 6 stress 0.08484704 ## ... Procrustes: rmse 1.008148e-06 max resid 2.038791e-06 ## ... Similar to previous best ## Run 7 stress 0.08484704 ## ... Procrustes: rmse 1.776536e-06 max resid 3.520974e-06 ## ... Similar to previous best ## Run 8 stress 0.09657264 ## Run 9 stress 0.08484704 ## ... Procrustes: rmse 8.550518e-07 max resid 1.794331e-06 ## ... Similar to previous best ## Run 10 stress 0.08484704 ## ... Procrustes: rmse 1.376679e-06 max resid 2.816876e-06 ## ... Similar to previous best ## Run 11 stress 0.08484704 ## ... Procrustes: rmse 4.702272e-06 max resid 8.17489e-06 ## ... Similar to previous best ## Run 12 stress 0.08484704 ## ... New best solution ## ... Procrustes: rmse 2.157179e-07 max resid 4.2813e-07 ## ... Similar to previous best ## Run 13 stress 0.08484704 ## ... Procrustes: rmse 1.726469e-06 max resid 3.270828e-06 ## ... Similar to previous best ## Run 14 stress 0.08484704 ## ... Procrustes: rmse 1.055175e-06 max resid 2.649077e-06 ## ... Similar to previous best ## Run 15 stress 0.09657265 ## Run 16 stress 0.1751066 ## Run 17 stress 0.08484704 ## ... Procrustes: rmse 6.953774e-07 max resid 1.374792e-06 ## ... Similar to previous best ## Run 18 stress 0.09584961 ## Run 19 stress 0.08484704 ## ... Procrustes: rmse 5.428812e-06 max resid 1.248684e-05 ## ... Similar to previous best ## Run 20 stress 0.1795526 ## *** Solution reached plot_ordination(ps_rare, ps_rare_bray, type=\u0026quot;samples\u0026quot;, color=\u0026quot;When\u0026quot;) + geom_point(size = 3)   Stacked bar plots plot_bar(ps, fill=\u0026quot;Phylum\u0026quot;) plot_bar(ps_relabund, fill=\u0026quot;Phylum\u0026quot;) + geom_bar(aes(color = Phylum, fill = Phylum), stat=\u0026quot;identity\u0026quot;, position=\u0026quot;stack\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Relative Abundance\\n\u0026quot;) + theme(panel.background = element_blank())  Heatmaps (ps_fam \u0026lt;- tax_glom(ps, \u0026quot;Family\u0026quot;)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 33 taxa and 19 samples ] ## sample_data() Sample Data: [ 19 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 33 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 33 reference sequences ] (ps_fam_rare \u0026lt;- rarefy_even_depth(ps_fam, sample.size = 4000, rngseed = 123, replace = FALSE)) ## `set.seed(123)` was used to initialize repeatable random subsampling. ## Please record this for your records so others can reproduce. ## Try `set.seed(123); .Random.seed` for the full vector ## ... ## 5 samples removedbecause they contained fewer reads than `sample.size`. ## Up to first five removed samples are: ## F3D142F3D143F3D144F3D146F3D5 ## ... ## 9OTUs were removed because they are no longer ## present in any sample after random subsampling ## ... ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 24 taxa and 14 samples ] ## sample_data() Sample Data: [ 14 samples by 4 sample variables ] ## tax_table() Taxonomy Table: [ 24 taxa by 7 taxonomic ranks ] ## refseq() DNAStringSet: [ 24 reference sequences ] plot_heatmap(ps_fam_rare, sample.label = \u0026quot;When\u0026quot;, taxa.label = \u0026quot;Family\u0026quot;) ## Warning: Transformation introduced infinite values in discrete y-axis   ","date":1564272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564367031,"objectID":"cec21b24eafe3bd9f42f5d7970a8e406","permalink":"/post/introduction-to-phyloseq/","publishdate":"2019-07-28T00:00:00Z","relpermalink":"/post/introduction-to-phyloseq/","section":"post","summary":"This post is from a tutorial demonstrating the processing of amplicon short read data in R taught as part of the Introduction to Metagenomics Summer Workshop. It provides a quick introduction some of the functionality provided by phyloseq and follows some of Paul McMurdie’s excellent tutorials. This tutorial picks up where Ben Callahan’s DADA2 tutorial leaves off and highlights some of the main accessor and processor functions of the package.","tags":["Microbiome","R","Phyloseq","DADA2"],"title":"Introduction to Phyloseq","type":"post"},{"authors":[],"categories":["Microbiome"],"content":"  \nThis post is also from the Introduction to Metagenomics Summer Workshop and provides a quick introduction to some common analytic methods used to analyze microbiome data. I thought it might be of interest to a broader audience so decided to post it here.\n\nThe goal of this session is to provide you with a high-level introduction to some common analytic methods used to analyze microbiome data. It will also serve to introduce you several popular R packages developed specifically for microbiome data analysis. We chose to emphasize R for this course because of the rapid development of methods and packages provided in the R language, the breadth of existing tutorials and resources, and the ever expanding community of R users. However, other platforms such as QIIME2, biobakery and USEARCH, just to name a few, offer excellent integrated solutions for the processing and analysis of amplicon and/or shotgun metagenomic sequence data.\nThe diverse goals and technical variation of metagenomic research projects does not allow for a standard “analytic pipeline” for microbiome data analysis. Approaching the analysis of microbiome data with a single workflow in mind is generally not a great idea, as there is no “one size fits all” solution for the assorted set of questions one might want to answer. However, you may be surprised to find that projects on very different topics often have overarching analytic aims such as:\n Describing the microbial community composition of a set of samples Estimating within- and between-sample diversity Identifying differentially abundant taxa Predicting a response from a set of taxonomic features Assessing microbial network structures and patterns of co-occurance Exploring the phylogenetic relatedness of a set of organisms  We will cover statistical methods developed to address several of these aims with a focus on introducing you to their implementation in R. A detailed description of each approach, its assumptions, package options, etc. is beyond the scope of this session. However, I try to provide links to source materials and more detailed documentation where possible. The statistical analysis of microbial metagenomic sequence data is a rapidly evolving field and different solutions (often many) have been proposed to answer the same questions. I have tried to focus on methods that are common in the microbiome literature, well-documented, and reasonably accessible…and a few I think are new and interesting. I also try to show a few different approaches in each section. In cases where I focus largely on more basic implementations, I have tried to provide links for advanced learning of more complex topics.\n\nThe publicly available data used in this session are from Giloteaux et. al. Reduced diversity and altered composition of the gut microbiome in individuals with myalgic encephalomyelitis/chronic fatigue syndrome published in Microbiome (2016). The metadata, OTU table, and taxonomy files were obtained from the QIIME2 tutorial Differential abundance analysis with gneiss (accessed on 06/13/2019). The code and data used to generate the phyloseq object is provided on my GitHub page. The data were generated by 16S rRNA gene sequencing (V4 hypervariable region) of fecal samples on the Illumina MiSeq. Our focus will be on examining differences in the microbiota of patients with chronic fatigue syndrome versus healthy controls. We will examine:\n Taxonomic relative abundance Hierarchal clustering Alpha-diversity Beta-diversity Differential abundance testing Predicting class labels  \n Additional resources There are many great resources for conducting microbiome data analysis in R. Statistical Analysis of Microbiome Data in R by Xia, Sun, and Chen (2018) is an excellent textbook in this area. For those looking for an end-to-end workflow for amplicon data in R, I highly recommend Ben Callahan’s F1000 Research paper Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses. In addition there are numerous websites and vignettes dedicated to microbiome analyses. A few include:\n Paul McMurdie’s phyloseq website Robert Edgar’s website The microbiome R package website All the materials and resources posted on the STAMPS wiki page (a course I highly recommend!)  \n Installing packages The code below will install the packages needed to run the analyses. These packages are installed from CRAN, Bioconductor and from developer GitHub sites. Several of these packages are large, and have many dependencies, so this will take some time. This code was modified from Ben’s Bioconductor paper.\nIn general, package management and versioning can be a challenge for those new to R. Inevitably, if you do not take steps ahead of time, you will find that one of your programs that ran fine just a few months ago, no longer works! Often this is because changes in new versions of packages or R caused your code to break. There are multiple solutions depending on your goals, and all come with pros and cons, but a good place to start is to learn more about Packrat and other package management tools.\nIf you already have many/some of these packages installed on your local system, you may want to skip this step and install manually only those that you need.\n.cran_packages \u0026lt;- c(\u0026quot;tidyverse\u0026quot;, \u0026quot;cowplot\u0026quot;, \u0026quot;picante\u0026quot;, \u0026quot;vegan\u0026quot;, \u0026quot;HMP\u0026quot;, \u0026quot;dendextend\u0026quot;, \u0026quot;rms\u0026quot;, \u0026quot;devtools\u0026quot;) .bioc_packages \u0026lt;- c(\u0026quot;phyloseq\u0026quot;, \u0026quot;DESeq2\u0026quot;, \u0026quot;microbiome\u0026quot;, \u0026quot;metagenomeSeq\u0026quot;, \u0026quot;ALDEx2\u0026quot;) .inst \u0026lt;- .cran_packages %in% installed.packages() if(any(!.inst)) { install.packages(.cran_packages[!.inst]) } if (!requireNamespace(\u0026quot;BiocManager\u0026quot;, quietly = TRUE)) install.packages(\u0026quot;BiocManager\u0026quot;) BiocManager::install(.bioc_packages, version = \u0026quot;3.9\u0026quot;) devtools::install_github(\u0026quot;adw96/breakaway\u0026quot;) devtools::install_github(repo = \u0026quot;UVic-omics/selbal\u0026quot;) \n Loading required packages Let’s load the required packages. This is not the most elegant way to do this, but it allows you to see each package that is loaded and the version number.\nlibrary(tidyverse); packageVersion(\u0026quot;tidyverse\u0026quot;)  ## [1] \u0026#39;1.2.1\u0026#39; library(phyloseq); packageVersion(\u0026quot;phyloseq\u0026quot;)  ## [1] \u0026#39;1.28.0\u0026#39; library(DESeq2); packageVersion(\u0026quot;DESeq2\u0026quot;)  ## [1] \u0026#39;1.24.0\u0026#39; library(microbiome); packageVersion(\u0026quot;microbiome\u0026quot;)  ## [1] \u0026#39;1.6.0\u0026#39; library(vegan); packageVersion(\u0026quot;vegan\u0026quot;)  ## [1] \u0026#39;2.5.5\u0026#39; library(picante); packageVersion(\u0026quot;picante\u0026quot;)  ## [1] \u0026#39;1.8\u0026#39; library(ALDEx2); packageVersion(\u0026quot;ALDEx2\u0026quot;)  ## [1] \u0026#39;1.16.0\u0026#39; library(metagenomeSeq); packageVersion(\u0026quot;metagenomeSeq\u0026quot;)  ## [1] \u0026#39;1.26.0\u0026#39; library(HMP); packageVersion(\u0026quot;HMP\u0026quot;)  ## [1] \u0026#39;1.6\u0026#39; library(dendextend); packageVersion(\u0026quot;dendextend\u0026quot;)  ## [1] \u0026#39;1.12.0\u0026#39; library(selbal); packageVersion(\u0026quot;selbal\u0026quot;)  ## [1] \u0026#39;0.1.0\u0026#39; library(rms); packageVersion(\u0026quot;rms\u0026quot;) ## [1] \u0026#39;5.1.3.1\u0026#39; library(breakaway); packageVersion(\u0026quot;breakaway\u0026quot;)  ## [1] \u0026#39;4.6.8\u0026#39; \n Reading in the Giloteaux data The data from the Giloteaux et. al. 2016 paper has been saved as a phyloseq object. We will use the readRDS() function to read it into R. We will also examine the distribution of read counts (per sample library size/read depth/total reads) and remove samples with \u0026lt; 5k total reads. We will then create a new metadata field “Status” that provides more “descriptive” values for our primary variable of interest; whether or not the sample was from a patient with chronic fatigue syndrome or a healthy control.\nThis should all be familiar to those of you who worked through the Introduction to Phyloseq session. However, something that will be new is that now we are using pipes from the magrittr package and tidyverse verbs to streamline some of the data manipulation steps. For those of you have not worked with the tidyverse set of packages and functions you are missing out! They will change they way you work in R. R for Data Science is an excellent source to learn more about the tidyverse packages and philosophy for data science.\nAdditional quality controls checks and data pre-processing specific to the goals of your project should be conducted at this point (but is outside of the scope of the current session).\n#Read in ps object (ps \u0026lt;- readRDS(\u0026quot;C:/Users/olljt2/Desktop/academic_web_page/static/data/ps_giloteaux_2016.rds\u0026quot;)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 138 taxa and 87 samples ] ## sample_data() Sample Data: [ 87 samples by 22 sample variables ] ## tax_table() Taxonomy Table: [ 138 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 138 tips and 137 internal nodes ] ## refseq() DNAStringSet: [ 138 reference sequences ] #Sort samples on total read count, remove \u0026lt;5k reads, remove any OTUs seen in only those samples sort(phyloseq::sample_sums(ps))  ## ERR1331827 ERR1331852 ERR1331856 ERR1331869 ERR1331833 ERR1331797 ## 2707 3031 3117 5083 5245 5307 ## ERR1331786 ERR1331818 ERR1331792 ERR1331803 ERR1331793 ERR1331819 ## 5696 5733 6463 6512 6622 6900 ## ERR1331858 ERR1331807 ERR1331815 ERR1331821 ERR1331843 ERR1331795 ## 6913 7121 7179 7272 7284 7314 ## ERR1331846 ERR1331811 ERR1331845 ERR1331842 ERR1331838 ERR1331855 ## 7569 7665 7815 7911 8102 8115 ## ERR1331824 ERR1331832 ERR1331804 ERR1331868 ERR1331831 ERR1331859 ## 8148 8186 8236 8612 8840 9016 ## ERR1331790 ERR1331789 ERR1331837 ERR1331857 ERR1331801 ERR1331841 ## 9085 9184 9731 9966 11173 11442 ## ERR1331861 ERR1331820 ERR1331854 ERR1331863 ERR1331806 ERR1331787 ## 11826 12940 13029 13094 13095 13690 ## ERR1331853 ERR1331851 ERR1331836 ERR1331835 ERR1331802 ERR1331799 ## 14113 14365 14488 14753 14799 14833 ## ERR1331847 ERR1331834 ERR1331817 ERR1331809 ERR1331828 ERR1331813 ## 15290 15367 15460 16162 16494 16749 ## ERR1331798 ERR1331816 ERR1331830 ERR1331785 ERR1331823 ERR1331865 ## 16947 17015 17457 17557 18506 19013 ## ERR1331848 ERR1331800 ERR1331867 ERR1331870 ERR1331810 ERR1331825 ## 19257 19443 19732 19783 19909 20069 ## ERR1331866 ERR1331871 ERR1331849 ERR1331860 ERR1331808 ERR1331872 ## 20760 20862 21540 21553 21713 22339 ## ERR1331812 ERR1331850 ERR1331791 ERR1331788 ERR1331796 ERR1331840 ## 22518 22639 23246 23751 23792 24752 ## ERR1331826 ERR1331822 ERR1331862 ERR1331864 ERR1331829 ERR1331844 ## 28186 28556 31064 44533 51918 57214 ## ERR1331805 ERR1331839 ERR1331794 ## 59355 61206 65941 (ps \u0026lt;- phyloseq::subset_samples(ps, phyloseq::sample_sums(ps) \u0026gt; 5000))  ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 138 taxa and 84 samples ] ## sample_data() Sample Data: [ 84 samples by 22 sample variables ] ## tax_table() Taxonomy Table: [ 138 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 138 tips and 137 internal nodes ] ## refseq() DNAStringSet: [ 138 reference sequences ] (ps \u0026lt;- phyloseq::prune_taxa(phyloseq::taxa_sums(ps) \u0026gt; 0, ps))  ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 138 taxa and 84 samples ] ## sample_data() Sample Data: [ 84 samples by 22 sample variables ] ## tax_table() Taxonomy Table: [ 138 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 138 tips and 137 internal nodes ] ## refseq() DNAStringSet: [ 138 reference sequences ] #Assign new sample metadata field phyloseq::sample_data(ps)$Status \u0026lt;- ifelse(phyloseq::sample_data(ps)$Subject == \u0026quot;Patient\u0026quot;, \u0026quot;Chronic Fatigue\u0026quot;, \u0026quot;Control\u0026quot;) phyloseq::sample_data(ps)$Status \u0026lt;- factor(phyloseq::sample_data(ps)$Status, levels = c(\u0026quot;Control\u0026quot;, \u0026quot;Chronic Fatigue\u0026quot;)) ps %\u0026gt;% sample_data %\u0026gt;% dplyr::count(Status) ## # A tibble: 2 x 2 ## Status n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Control 37 ## 2 Chronic Fatigue 47 We can see that we have a phyloseq object consisting of 138 taxa on 84 samples, 22 sample metadata fields, 7 taxonomic ranks and that a phylogenetic tree and the reference sequences have been included. We also see that there are data on n=37 controls and n=47 patients with chronic fatigue.\n\n Visualizing relative abundance Often an early step in many microbiome projects to visualize the relative abundance of organisms at specific taxonomic ranks. Stacked bar plots and faceted box plots are two ways of doing this. I recommend that if using bar plots to include each sample as a separate observation (and not to aggregate by groups). This is because the sample-to-sample variability can be high, even within groups, which may be just or more important to observe than between-group differences…which can be obscured with aggregation.\nThe ability to discriminate between more than say a dozen colors in a single plot is also a limitation of the stacked bar plot (faceted box plots do not suffer this limitation). Thus, this is one analysis I often run in QIIME2 using the taxa barplot command, as it allows for beautiful interactive viewing. This could also be done in R using a shiny app. I just haven’t implemented, or seen others implement, this functionality yet in R (I imagine someone has so please let me know if/when you do).\nHere we will agglomerate the reads to the phylum-level using phyloseq and plot the relative abundance by Status.\n#Get count of phyla table(phyloseq::tax_table(ps)[, \u0026quot;Phylum\u0026quot;]) ## ## Actinobacteria Bacteroidetes Cyanobacteria Euryarchaeota ## 7 11 2 1 ## Firmicutes Fusobacteria Proteobacteria Tenericutes ## 105 1 7 2 ## Verrucomicrobia ## 1 #Convert to relative abundance ps_rel_abund = phyloseq::transform_sample_counts(ps, function(x){x / sum(x)}) phyloseq::otu_table(ps)[1:5, 1:5] ## OTU Table: [5 taxa and 5 samples] ## taxa are rows ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ## OTU1 2 581 347 916 10498 ## OTU2 371 46 0 233 301 ## OTU3 1189 81 637 199 0 ## OTU4 0 172 246 0 372 ## OTU5 308 44 143 155 221 phyloseq::otu_table(ps_rel_abund)[1:5, 1:5] ## OTU Table: [5 taxa and 5 samples] ## taxa are rows ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ## OTU1 0.0003020236 0.026008326 0.05028986 0.013891206 0.73080404 ## OTU2 0.0560253700 0.002059179 0.00000000 0.003533462 0.02095371 ## OTU3 0.1795530051 0.003625946 0.09231884 0.003017849 0.00000000 ## OTU4 0.0000000000 0.007699539 0.03565217 0.000000000 0.02589628 ## OTU5 0.0465116279 0.001969649 0.02072464 0.002350586 0.01538462 #Plot phyloseq::plot_bar(ps_rel_abund, fill = \u0026quot;Phylum\u0026quot;) + geom_bar(aes(color = Phylum, fill = Phylum), stat = \u0026quot;identity\u0026quot;, position = \u0026quot;stack\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Relative Abundance\\n\u0026quot;) + facet_wrap(~ Status, scales = \u0026quot;free\u0026quot;) + theme(panel.background = element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) There are a total of nine phyla and their relative abundance looks to be quite simialr between groups. You could sort the taxa on abundance to improve the vizualization. I’ll let you give that a shot on your own. Let’s generate box plots according to group and facet them by phylum using the raw counts. We will use the phyloseq::melt function to help.\n\n#Agglomerate to phylum-level and rename ps_phylum \u0026lt;- phyloseq::tax_glom(ps, \u0026quot;Phylum\u0026quot;) phyloseq::taxa_names(ps_phylum) \u0026lt;- phyloseq::tax_table(ps_phylum)[, \u0026quot;Phylum\u0026quot;] phyloseq::otu_table(ps_phylum)[1:5, 1:5] ## OTU Table: [5 taxa and 5 samples] ## taxa are rows ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ## Bacteroidetes 1903 878 1837 1969 11776 ## Proteobacteria 119 3315 468 62358 319 ## Firmicutes 4319 14429 3548 1609 2207 ## Actinobacteria 30 976 17 0 58 ## Cyanobacteria 246 0 0 0 0 #Melt and plot phyloseq::psmelt(ps_phylum) %\u0026gt;% ggplot(data = ., aes(x = Status, y = Abundance)) + geom_boxplot(outlier.shape = NA) + geom_jitter(aes(color = OTU), height = 0, width = .2) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Abundance\\n\u0026quot;) + facet_wrap(~ OTU, scales = \u0026quot;free\u0026quot;) As we saw before, many samples have a high number of Firmicutes, followed by Bacteroidetes, and Actinobacteria. Most samples have low read counts for other phyla with some outlying samples. There does not appear to be much difference in the major phyla between groups. Check out Ben Callahan’s F1000 paper for additional examples on visualizing sequence variant prevalence/abundance that may be helpful for specific analyses.\nOne way to formally test for a difference in the phylum-level abundance is to conduct a multivariate test for differences in the overall composition between groups of samples. This type of test can be implemented using the HMP package (Xdc.sevsample function) described in the paper Hypothesis Testing and Power Calculations for Taxonomic-Based Human Microbiome Data by La Rosa et. al.\nBasically, a Dirichlet-Multinomial distribution is assumed for the data and null hypothesis testing is conducted by testing for a difference in the location (mean distribution of each taxa) across groups accounting for the overdispersion in the count data. The authors describe this test as analogous to a two sample t-test, but instead we are evaluating whether taxa frequencies observed in both groups of metagenomic samples are equal (null hypothesis). Here we are performing the test on bacterial phyla, but it could be performed at any taxonomic level including OTUs. The authors recommend that rare taxa be pooled into a single group to improve testing.\n#Subset groups controls \u0026lt;- phyloseq::subset_samples(ps_phylum, Status == \u0026quot;Control\u0026quot;) cf \u0026lt;- phyloseq::subset_samples(ps_phylum, Status == \u0026quot;Chronic Fatigue\u0026quot;) #Output OTU tables control_otu \u0026lt;- data.frame(phyloseq::otu_table(controls)) cf_otu \u0026lt;- data.frame(phyloseq::otu_table(cf)) #Group rare phyla control_otu \u0026lt;- control_otu %\u0026gt;% t(.) %\u0026gt;% as.data.frame(.) %\u0026gt;% mutate(Other = Cyanobacteria + Euryarchaeota + Tenericutes + Verrucomicrobia + Fusobacteria) %\u0026gt;% dplyr::select(-Cyanobacteria, -Euryarchaeota, -Tenericutes, -Verrucomicrobia, -Fusobacteria) cf_otu \u0026lt;- cf_otu %\u0026gt;% t(.) %\u0026gt;% as.data.frame(.) %\u0026gt;% mutate(Other = Cyanobacteria + Euryarchaeota + Tenericutes + Verrucomicrobia + Fusobacteria) %\u0026gt;% dplyr::select(-Cyanobacteria, -Euryarchaeota, -Tenericutes, -Verrucomicrobia, -Fusobacteria) #HMP test group_data \u0026lt;- list(control_otu, cf_otu) (xdc \u0026lt;- HMP::Xdc.sevsample(group_data))  ## $`Xdc statistics` ## [1] 0.2769004 ## ## $`p value` ## [1] 0.9980551 1 - pchisq(.2769004, 5) ## [1] 0.9980551 The HMP test fails to reject the null hypothesis of no difference in the distribution of phyla between groups (in line with our expectations). The xdc test follows a Chi-square distribution with degrees of freedom equal to (J-1)*K, where J is the number of groups and K is the number of taxa. The last calculation just shows how the p-value is obtained. The test can be expanded to more than two groups and to test for differences in rank abundance distributions (RAD). These are topics I encourage you to explore on your own. The microbiome package also has some nice functions for visualizing community composition you should look into.\n\n Hierarchical clustering Another early step in many microbiome projects to examine how samples cluster on some measure of taxonomic (dis)similarity. There are MANY ways to do perform such clustering. Here I present just one approach that I assume many of you are familiar with. We will perform hierarchal clustering of samples based on their Bray-Curtis dissimilarity. Here is a link to how it is calculated. We will discuss this in more detail during the lecture, but for now it should suffice to know that the as two samples share fewer taxa, the number increases. The Bray-Curtis dissimilarity is zero for samples that have the exact same composition and one for those sharing no taxa. It is also worth remembering that this is a measure of dissimilarity (it is not a true distance measure).\nWe will use the popular vegan package for community ecology to compute the Bray-Curtis dissimilarity for all samples. Then we will apply Ward’s clustering and color code the sample names to assess the extent to which the samples from the control and chronic fatigue participants cluster. At a high-level, Ward’s clustering finds the pair of clusters at each iteration that minimalizes the increase in total variance.\nLet’s see how this is done in R.\n#Extract OTU table and compute BC ps_rel_otu \u0026lt;- data.frame(phyloseq::otu_table(ps_rel_abund)) ps_rel_otu \u0026lt;- t(ps_rel_otu) bc_dist \u0026lt;- vegan::vegdist(ps_rel_otu, method = \u0026quot;bray\u0026quot;) as.matrix(bc_dist)[1:5, 1:5] ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ## ERR1331793 0.0000000 0.8801040 0.5975550 0.9767218 0.8684629 ## ERR1331872 0.8801040 0.0000000 0.7590766 0.9596181 0.9206484 ## ERR1331819 0.5975550 0.7590766 0.0000000 0.9556656 0.7810736 ## ERR1331794 0.9767218 0.9596181 0.9556656 0.0000000 0.9693291 ## ERR1331851 0.8684629 0.9206484 0.7810736 0.9693291 0.0000000 #Save as dendrogram ward \u0026lt;- as.dendrogram(hclust(bc_dist, method = \u0026quot;ward.D2\u0026quot;)) #Provide color codes meta \u0026lt;- data.frame(phyloseq::sample_data(ps_rel_abund)) colorCode \u0026lt;- c(Control = \u0026quot;red\u0026quot;, `Chronic Fatigue` = \u0026quot;blue\u0026quot;) labels_colors(ward) \u0026lt;- colorCode[meta$Status][order.dendrogram(ward)] #Plot plot(ward) We can see that the Bray-Curtis dissimilarity for these selected samples range from around 0.6 to close to 1. Thus, the composition of some samples are quite different from one another. We also see some clustering according to Status near the tips, but no clear “higher-level” clustering. We will try to exploit this information later to see if we can predict the label of each sample with only information on the microbial relative abundances.\nHeatmaps are another good way to visualize these types of associations and can be implemented using phyloseq. Give it a try on your own!\n\n Alpha-diversity Robert Edgar provides an excellent definition of alpha-diversity on his website:\n Alpha-diversity is the diversity in a single ecosystem or sample. The simplest measure is richness, the number of species (or OTUs) observed in the sample. Other metrics consider the abundances (frequencies) of the OTUs, for example to give lower weight to lower-abundance OTUs.\n Basically, it is the within-sample diversity and includes how many organisms are observed (i.e. observed OTUs) and how evenly they are distributed. Many researchers are interested in estimating alpha-diversity since differences between groups have been associated with several health related outcomes. However the issue of how to best estimate these quantities using data derived from next-generation sequencing (NGS) is controversial. This is due to two main reasons:\nThe observed richness in a sample/site is typically underestimated due to inexhaustive sampling. Thus, valid estimators of diversity require extrapolating from the available observations to provide estimates of the unobserved taxa (and also account for the sampling variability). Extrapolation estimators require an accurate count of the rare taxa (including singletons) in each sample…which for NGS-based metagenomics studies we typically do not have…since singletons generally cannot be differentiated from sequencing errors using the current best informatics workflows. The extent to which we cannot accurately detect low abundance taxa limits the utility of diversity estimators reliant upon such counts.  So we are kind of in a catch-22 regarding the best way forward given current technologies.\nIt has been argued; however, that diversity metrics can nevertheless be compared between samples because the errors and biases are mostly systematic (i.e. occur with similar rates in all samples). See Dr. Edgar’s discussion of the topic here for more detail. This is what is typically done in most published studies to date. A major underlying assumption here is that abundance structures are the same for the two groups being compared. This is perhaps a reasonable assumption when comparing similar environments, but it is hard to know without exhaustive sampling. See figure 1 here and the related discussion by Amy Willis for a more detailed understanding of how the abundance structure can lead you to incorrect conclusions (quite disconcerting).\nRarefaction (subsampling reads from each sample without replacement to a constant depth) is often performed before estimating alpha-diversity; although, it is unclear to me if/when this helps since environments can be identical with respect to one alpha diversity metric, but the different abundance structures will induce different biases when rarified (italicized text taken from Amy’s paper linked to above).\nDr. Willis has examined this issue in depth and developed breakaway and DivNet to specifically address the shortcoming of current approaches. I highly recommend you check out her GitHub site. In a recent paper she argues:\n In order to draw meaningful conclusions about the entire microbial community, it is necessary to adjust for inexhaustive sampling using statistically-motivated parameter estimates for alpha diversity. In order to draw meaningful conclusions regarding comparisons of microbial communities, it is necessary to use measurement error models to adjust for the uncertainty in the estimation of alpha diversity. She also states that breakaway is not overly sensitive to singleton counts.\n The links below provide a brief introduction to the topic. I look forward to Amy’s updated tutorial and thoughts on when microbial diversity estimation is, and isn’t, possible as mentioned in the last link (I suspect it will result in some updating of these materials).\n https://www.drive5.com/usearch/manual/alpha_diversity.html https://www.biorxiv.org/content/biorxiv/early/2017/12/11/231878.full.pdf https://github.com/benjjneb/dada2/issues/103 https://github.com/benjjneb/dada2/issues/317  \nBelow we will estimate and test for differences according to chronic fatigue status using the plug-in estimates for observed richness, Shannon diversity, and phylogenetic diversity on the subsampled data (since this is common practice). I have also provided some code to estimate richness using breakaway that you can examine on your own. I plan to update this section with some data that is more appropriate for breakaway. So check back soon.\nggplot(data = data.frame(\u0026quot;total_reads\u0026quot; = phyloseq::sample_sums(ps), \u0026quot;observed\u0026quot; = phyloseq::estimate_richness(ps, measures = \u0026quot;Observed\u0026quot;)[, 1]), aes(x = total_reads, y = observed)) + geom_point() + geom_smooth(method=\u0026quot;lm\u0026quot;, se = FALSE) + labs(x = \u0026quot;\\nTotal Reads\u0026quot;, y = \u0026quot;Observed Richness\\n\u0026quot;) \nWe see that the observed OTUs are correlated with the total read count (as expected). Now let’s subsample, plot, and test for group differences.\n#Subsample reads (ps_rare \u0026lt;- phyloseq::rarefy_even_depth(ps, rngseed = 123, replace = FALSE))  ## `set.seed(123)` was used to initialize repeatable random subsampling. ## Please record this for your records so others can reproduce. ## Try `set.seed(123); .Random.seed` for the full vector ## ... ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 138 taxa and 84 samples ] ## sample_data() Sample Data: [ 84 samples by 23 sample variables ] ## tax_table() Taxonomy Table: [ 138 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 138 tips and 137 internal nodes ] ## refseq() DNAStringSet: [ 138 reference sequences ] head(phyloseq::sample_sums(ps_rare)) ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ERR1331834 ## 5083 5083 5083 5083 5083 5083 #Generate a data.frame with adiv measures adiv \u0026lt;- data.frame( \u0026quot;Observed\u0026quot; = phyloseq::estimate_richness(ps_rare, measures = \u0026quot;Observed\u0026quot;), \u0026quot;Shannon\u0026quot; = phyloseq::estimate_richness(ps_rare, measures = \u0026quot;Shannon\u0026quot;), \u0026quot;PD\u0026quot; = picante::pd(samp = data.frame(t(data.frame(phyloseq::otu_table(ps_rare)))), tree = phyloseq::phy_tree(ps_rare))[, 1], \u0026quot;Status\u0026quot; = phyloseq::sample_data(ps_rare)$Status) head(adiv) ## Observed Shannon PD Status ## ERR1331793 53 2.7462377 20.597980 Chronic Fatigue ## ERR1331872 52 2.7527053 21.289719 Control ## ERR1331819 70 3.2378006 21.671340 Control ## ERR1331794 27 0.3761523 8.275154 Chronic Fatigue ## ERR1331851 45 1.3387308 14.783592 Chronic Fatigue ## ERR1331834 54 2.8883445 20.988640 Control #Plot adiv measures adiv %\u0026gt;% gather(key = metric, value = value, c(\u0026quot;Observed\u0026quot;, \u0026quot;Shannon\u0026quot;, \u0026quot;PD\u0026quot;)) %\u0026gt;% mutate(metric = factor(metric, levels = c(\u0026quot;Observed\u0026quot;, \u0026quot;Shannon\u0026quot;, \u0026quot;PD\u0026quot;))) %\u0026gt;% ggplot(aes(x = Status, y = value)) + geom_boxplot(outlier.color = NA) + geom_jitter(aes(color = Status), height = 0, width = .2) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;) + facet_wrap(~ metric, scales = \u0026quot;free\u0026quot;) + theme(legend.position=\u0026quot;none\u0026quot;) #Summarize adiv %\u0026gt;% group_by(Status) %\u0026gt;% dplyr::summarise(median_observed = median(Observed), median_shannon = median(Shannon), median_pd = median(PD)) ## # A tibble: 2 x 4 ## Status median_observed median_shannon median_pd ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Control 49 2.40 18.1 ## 2 Chronic Fatigue 46 2.30 17.0 #Wilcoxon test of location wilcox.test(Observed ~ Status, data = adiv, exact = FALSE, conf.int = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Observed by Status ## W = 1007.5, p-value = 0.2146 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -1.000059 5.000002 ## sample estimates: ## difference in location ## 2.000087 wilcox.test(Shannon ~ Status, data = adiv, conf.int = TRUE)  ## ## Wilcoxon rank sum test ## ## data: Shannon by Status ## W = 1037, p-value = 0.1329 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -0.04346366 0.39218192 ## sample estimates: ## difference in location ## 0.1421467 wilcox.test(PD ~ Status, data = adiv, conf.int = TRUE) ## ## Wilcoxon rank sum test ## ## data: PD by Status ## W = 995, p-value = 0.2616 ## alternative hypothesis: true location shift is not equal to 0 ## 95 percent confidence interval: ## -0.7452342 2.1356893 ## sample estimates: ## difference in location ## 0.6854015 Here we see a modestly lower median alpha-diversity in samples from participants with chronic fatigue when compared to healthy controls. However, the variation in alpha-diversity between groups is highly overlapping and we fail to reject the null hypothesis of no difference in location between groups.\n\nBelow is the code to estimate richness using breakaway. You will see some warnings. I plan to update this section with some additional data so check back soon.\n#Obtain breakaway estimates ba_adiv \u0026lt;- breakaway::breakaway(ps) ba_adiv[1] #Plot estimates plot(ba_adiv, ps, color = \u0026quot;Status\u0026quot;) #Examine models summary(ba_adiv) %\u0026gt;% add_column(\u0026quot;SampleNames\u0026quot; = ps %\u0026gt;% otu_table %\u0026gt;% sample_names) #Test for group differnce bt \u0026lt;- breakaway::betta(summary(ba_adiv)$estimate, summary(ba_adiv)$error, make_design_matrix(ps, \u0026quot;Status\u0026quot;)) bt$table  \n Beta-diversity Beta-diversity provides a measure of similarity, or dissimilarity, of one microbial composition to another. Beta-diversity is typically calculated on the OTU/ASV/species composition tables directly (after normalization), but can be calculated using abundances at higher taxonomic levels. One common estimator of microbial beta-diversity is the pairwise Euclidean distance between samples. However, many ecologically informative measures are also commonly used and include:\n Bray-Curtis similarity Jaccard similarity Yue \u0026amp; Clayton theta similarity UniFrac distance AND MANY MORE  Pat Schloss provides a listing and links to a large number of alpha- and beta-diversity estimators on his mothur wiki page. He also offers workshops on using mothur for processing amplicon sequence data and on using R for microbial ecologists a few times a year that I highly recommend.\nThis is probably a good time to touch on count normalization. One of the challenges we face working with NGS-derived sequence data is that the total number of reads for each sample is not directly tied to the starting quantity of DNA. You can think of the total reads (to a reasonable approximation) as getting assigned by a random sampling process where some samples just get doled out more reads. Thus, the total count does not carry any information on the absolute abundance of taxa. As long as the count is sufficiently large, it is just a factor that we want to account for in our analysis and is not of particular interest other than differences across samples can be a source of bias. Paul McMurdie provides an excellent discussion of the various goals and some approaches for normalization in his chapter on Normalization of Microbiome Profiling Data in the first edition of Microbiome Analysis. Weiss et. al. also provide a great introduction and examination of the impact of normalization approaches on beta-diversity ordinations and differential abundance testing.\nHere we will consider two approaches for library size normalization. The first will employ a compositional data analysis approach and involves working with log-ratios. The second will involve simply subsampling the data without replacement; however, this approach comes with limiations. We will use it here as the authors of the UniFrac method have suggested that rarefying more clearly clusters samples according to biological origin than other normalization techniques do for ordination metrics based on presence or absence (i.e. unweighted UniFrac).\nA detailed discussion of compositional data analysis (CoDA) is beyond the scope of this session. I plan to add a tutorial devoted to CoDA in the future so check back. At a high-level compositional data (i.e. data that carry only relative information and are constrained by a unit sum) exist in a restricted subspace of the Euclidian geometry referred to as the D-1 simplex (I know this doesn’t feel high-level). Due to this constraint, these data fail to meet many of the assumptions of our favorite statistical methods developed for unconstrained random variables. Working with ratios of compositional elements lets us transform these data to the Euclidian space and apply our favorite methods (so we don’t need to work in the simplex). Working with their logarithms makes them easier to interpret. There are different types of log-ratio “transformations” including the additive log-ratio, centered log-ratio, and isometric log-ratio transforms. Below are some great resources for learning more about compositional data analysis:\n*Understanding sequencing data as compositions: an outlook and review by Quinn et. al. in Bioinformatics (2018)\n*Statistical Analysis of Microbiome Data with R - Ch. 10\n*Applied Compositional Data Analysis by Filzmoser, Hron, and Templ (2018)\n*Analyzing Compositional Data with R by Boogaart and Tolosana-Delgado (2013)\n\nBelow we generate a beta-diversity ordination using the Aitchison distance. This is simply applying PCA to the centered log-ratio (CLR) transformed counts. We will use the microbiome package to do this and assign a pseudocount of 1 to facilitate the transformation (since the log of zero is undefined). There are alternative/better approaches than using a pseudocount and we will examine one in the next section. First we perform the transformation.\n#CLR transform (ps_clr \u0026lt;- microbiome::transform(ps, \u0026quot;clr\u0026quot;))  ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 138 taxa and 84 samples ] ## sample_data() Sample Data: [ 84 samples by 23 sample variables ] ## tax_table() Taxonomy Table: [ 138 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 138 tips and 137 internal nodes ] ## refseq() DNAStringSet: [ 138 reference sequences ] phyloseq::otu_table(ps)[1:5, 1:5] ## OTU Table: [5 taxa and 5 samples] ## taxa are rows ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ## OTU1 2 581 347 916 10498 ## OTU2 371 46 0 233 301 ## OTU3 1189 81 637 199 0 ## OTU4 0 172 246 0 372 ## OTU5 308 44 143 155 221 phyloseq::otu_table(ps_clr)[1:5, 1:5] ## OTU Table: [5 taxa and 5 samples] ## taxa are rows ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ## OTU1 1.289544 5.812706 5.615063 6.230204 9.467837 ## OTU2 6.485240 3.280355 -3.079591 4.863001 5.916398 ## OTU3 7.649802 3.844401 6.222432 4.705673 -1.903003 ## OTU4 -2.317399 4.596219 5.271139 -1.178342 6.128105 ## OTU5 6.299168 3.236089 4.728822 4.456584 5.607596 We can see that the values are now no longer counts, but rather the dominance (or lack thereof) for each taxa relative to the geometric mean of all taxa on the logarithmic scale (any log base could be used and often log2 or log10 may aid in interpretation).\n\nNow we will conduct the PCA, examine the relative importance of each principal component, and generate the ordination. PCA is an unsupervised learning approach that can help us see similarities between samples when there are a large number of features. Scatter plots are not much help here in high-dimensions since the number of possible plots is equal to p(p-1)2 where p = the number of features (quickly becomes intractable). So we need to find an approach that will let us map these data to a lower-dimensional space. This is what PCA does. It identifies latent variables referred to as principal components (PC) that capture as much of the information as possible…where information is the amount of variation in the data. We can then focus on those PCs that are most interesting (i.e. explain the most variation; give us the best lower-dimensional mapping). Given we can only visualize our samples in 2- or 3-dimenstional space, most microbiome studies only plot the data using either the first couple of PCs. A more though introduction to PCA can be found in the textbook An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani (2013). Let’s give it a try!\n#PCA via phyloseq ord_clr \u0026lt;- phyloseq::ordinate(ps_clr, \u0026quot;RDA\u0026quot;) #Plot scree plot phyloseq::plot_scree(ord_clr) + geom_bar(stat=\u0026quot;identity\u0026quot;, fill = \u0026quot;blue\u0026quot;) + labs(x = \u0026quot;\\nAxis\u0026quot;, y = \u0026quot;Proportion of Variance\\n\u0026quot;) #Examine eigenvalues and % prop. variance explained head(ord_clr$CA$eig)  ## PC1 PC2 PC3 PC4 PC5 PC6 ## 75.69204 36.27003 33.16649 29.08833 25.52986 24.32215 sapply(ord_clr$CA$eig[1:5], function(x) x / sum(ord_clr$CA$eig))  ## PC1 PC2 PC3 PC4 PC5 ## 0.10744095 0.05148344 0.04707812 0.04128939 0.03623832 RDA without constraints is PCA…and we can generate the PCs using the phyloseq::ordinate function. A scree plot is then used to examine the proportion of total variation explained by each PC. Here we see that the first PC really stands out and then we have a gradual decline for the remaining components. You may hear people talk about looking for the “elbow” in the plot where the information plateaus to select the number of PCs to retain. Below we plot the first two components and scale the plot to reflect the relative amount of information explained by each axis as recommended by Nguyen and Holmes in their paper Ten quick tips for effective dimensionality reduction.\n\n#Scale axes and plot ordination clr1 \u0026lt;- ord_clr$CA$eig[1] / sum(ord_clr$CA$eig) clr2 \u0026lt;- ord_clr$CA$eig[2] / sum(ord_clr$CA$eig) phyloseq::plot_ordination(ps, ord_clr, type=\u0026quot;samples\u0026quot;, color=\u0026quot;Status\u0026quot;) + geom_point(size = 2) + coord_fixed(clr2 / clr1) + stat_ellipse(aes(group = Status), linetype = 2) We see some separation between the chronic fatigue and healthy controls samples suggesting some differences in the communities according to sample type. There is also a fair degree of overlap as is often seen in clinical research studies examining the same environment in two different patient populations. While PCA is an exploratory data visualization tool, we can test whether the samples cluster beyond that expected by sampling variability using permutational multivariate analysis of variance (PERMANOVA). It does this by partitioning the sums of squares for the within- and between-cluster components using the concept of centroids. Many permutations of the data (i.e. random shuffling) are used to generate the null distribution. The test from ADONIS can be confounded by differences in dispersion (or spread)…so we want to check this as well. #Generate distance matrix clr_dist_matrix \u0026lt;- phyloseq::distance(ps_clr, method = \u0026quot;euclidean\u0026quot;) #ADONIS test vegan::adonis(clr_dist_matrix ~ phyloseq::sample_data(ps_clr)$Status) ## ## Call: ## vegan::adonis(formula = clr_dist_matrix ~ phyloseq::sample_data(ps_clr)$Status) ## ## Permutation: free ## Number of permutations: 999 ## ## Terms added sequentially (first to last) ## ## Df SumsOfSqs MeanSqs F.Model R2 ## phyloseq::sample_data(ps_clr)$Status 1 2240 2240.17 3.2666 0.03831 ## Residuals 82 56233 685.77 0.96169 ## Total 83 58473 1.00000 ## Pr(\u0026gt;F) ## phyloseq::sample_data(ps_clr)$Status 0.001 *** ## Residuals ## Total ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 #Dispersion test and plot dispr \u0026lt;- vegan::betadisper(clr_dist_matrix, phyloseq::sample_data(ps_clr)$Status) dispr ## ## Homogeneity of multivariate dispersions ## ## Call: vegan::betadisper(d = clr_dist_matrix, group = ## phyloseq::sample_data(ps_clr)$Status) ## ## No. of Positive Eigenvalues: 83 ## No. of Negative Eigenvalues: 0 ## ## Average distance to median: ## Control Chronic Fatigue ## 25.1 26.2 ## ## Eigenvalues for PCoA axes: ## (Showing 8 of 83 eigenvalues) ## PCoA1 PCoA2 PCoA3 PCoA4 PCoA5 PCoA6 PCoA7 PCoA8 ## 6282 3010 2753 2414 2119 2019 1895 1693 plot(dispr, main = \u0026quot;Ordination Centroids and Dispersion Labeled: Aitchison Distance\u0026quot;, sub = \u0026quot;\u0026quot;) boxplot(dispr, main = \u0026quot;\u0026quot;, xlab = \u0026quot;\u0026quot;) permutest(dispr) ## ## Permutation test for homogeneity of multivariate dispersions ## Permutation: free ## Number of permutations: 999 ## ## Response: Distances ## Df Sum Sq Mean Sq F N.Perm Pr(\u0026gt;F) ## Groups 1 24.95 24.9463 3.0491 999 0.075 . ## Residuals 82 670.89 8.1816 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 We reject the null hypothesis of no difference in the centroid location according to Status. However, the proportion of variance explained is quite small. You might get slightly different numbers. This is because of the random process generating the permutations. There is a suggestion that the dispersion is greater in samples from patients with chronic fatigue syndrome. However, it does not exceed that expected by sampling variablilty at this sample size. As has been explained by others (Xia, Sun, and Chen; Ch 7.4), I want to mention that this type of testing is akin to attempting to “explain” the axes using metadata fields. A more formal approach to hypotheses testing can be done using redundancy analysis or canonical correspondence analysis that directly uses information on metadata fields when generating the ordinations and conducting testing. These approaches directly test hypotheses about environmental variables. I will not demonstrate these approaches here, but they can be computed using some of these same commands with minor modifications.\nLastly, I want to show you how you can bring in information the form of a phylogenic tree into beta-diversity analysis. The UniFrac metric incorporates phylogenic information by calculating the total branch lengths “unshared” between two samples divided by the total branch length. This approach often reveals interesting differences in the phylogenic relatedness between samples and sample types. Here we compute the weighted and unweighted UniFrac metrics using PCoA. PCoA can be thought of as PCA for non-Euclidian measures.\n#Generate distances ord_unifrac \u0026lt;- ordinate(ps_rare, method = \u0026quot;PCoA\u0026quot;, distance = \u0026quot;wunifrac\u0026quot;) ord_unifrac_un \u0026lt;- ordinate(ps_rare, method = \u0026quot;PCoA\u0026quot;, distance = \u0026quot;unifrac\u0026quot;) #Plot ordinations a \u0026lt;- plot_ordination(ps_rare, ord_unifrac, color = \u0026quot;Status\u0026quot;) + geom_point(size = 2) b \u0026lt;- plot_ordination(ps_rare, ord_unifrac_un, color = \u0026quot;Status\u0026quot;) + geom_point(size = 2) cowplot::plot_grid(a, b, nrow = 1, ncol = 2, scale = .9, labels = c(\u0026quot;Weighted\u0026quot;, \u0026quot;Unweighted\u0026quot;)) There is a large amount of overlap between sample types for the weighted UniFrac distance (accounts for the relative abundance of each of the taxa within the communities). However, there is clustering on at least the first axis for the unweighted UniFrac distance that is not “explained” by Status. Is there a metadata field in the data that reflects this separation? I’ll let you explore on your own.\n\n Differential abundance testing The goal of differential abundance testing is to identify specific taxa associated with clinical metadata variables of interest. This is a difficult task. It is also one of the more controversial areas in microbiome data analysis. Some of the reasons for this are described in a recent paper by James Morton et. al. in Nature Communications (2019), but is related to concerns that normalization and testing approaches have generally failed to control false discovery rates (here is a good example) and this has contributed to the lack of reproducibility in microbiome studies. If you think about it for a moment, a couple of difficulties come to mind:\n The goal of this type of analysis is to identify taxa that differ the most between conditions (or along a continuous gradient). Basically, we are identifying the most extreme results in the data. We would therefore expect some/many/most of these findings to have been “outlying” results simply due to chance sampling variation and to perhaps regress back towards the mean/null value when tested in a new sample of patients. The data are compositional and thus changes in one or more taxa can make it look like other/all taxa are changing. James Morton has an excellent example of this here. Methods that don’t properly account of the compositional nature of the data can have very high false discovery rates.. Functionally redundant taxa may serve the same “niche” in different environments or populations causing different taxa to be identified as differentially abundant across samples (however the testing approach would not be what is misleading here). The high correlation between many taxa may cause different, but highly correlated, features to be selected in different studies.  Professor Frank Harrell provides a great overview of this general concern in Chapter 20 of his Biostatistics for Biomedical Research online text. For general thoughts on statistics and predictive modeling I highly recommend that you check out his blog and regression modeling strategies course notes.\nOther fields have wrestled with similar problems and have introduced approaches such as the requirement of replicating results in multiple cohorts of patients prior to publication (or at least employing rigorous resampling approaches to gauge the reproducibility), analysis pre-specification, and focusing more on prediction than “naming names”. For compositional data including external information in the form of external spike-ins or estimates of total abundance (such as estimating total microbial load using qPCR), working with ratios, limiting the emphasis on testing, and understanding the limits of compositional data are likely reasonable ways forward here. However, none of these are a panacea. Methodologists working in the area of microbiome data analysis are addressing some of these issues, but there is still much work to be done. Two excellent recent papers you should check out include James Morton’s paper above and this preprint in biorxiv by McLaren, Willis and Callahan (2019) explaining and modeling correctable bias in metagenomic sequence studies.\nIn this section, I will present a two approaches for estimating differential abundance. The first is simply applying the non-parametric Wilcoxon rank-sum test to each taxon. The second is a version of the Wilcoxon test developed for compositional NGS data. I chose these two approaches since they are commonly used in microbiome studies and I expect many of you will have some familiarity with the Wilcoxon test or (Gosset’s) t-test. However, all results should be interpreted in light of the concerns raised above. I also include the use of a CoDA transform for both since there does seem to be some growing support that log-ratio methodologies may better control the false positive rate.\nMany researchers will apply the non-parametric Wilcoxon rank-sum test to each OTU/ASV/species after normalization. We will do this here. We will also use nested data frames as advocated by Hadley Wickham to keep the data and test results together in a single data.frame. At first, I found this approach a little strange. However, I have come to use it all the time. It is perhaps a bit overkill here, but a very helpful framework when you want to run many models and then save them together with the data and results (especially when they take a long time to run). Here is a link to a complete description of the nested frame approach in the R for Data Science book. We also use the map function from purr. This operates like a for loop, allowing us to iterate the test over each OTU, but with less coding. This is a big chunk of code. I will talk you through it.\n#Generate data.frame with OTUs and metadata ps_wilcox \u0026lt;- data.frame(t(data.frame(phyloseq::otu_table(ps_clr)))) ps_wilcox$Status \u0026lt;- phyloseq::sample_data(ps_clr)$Status #Define functions to pass to map wilcox_model \u0026lt;- function(df){ wilcox.test(abund ~ Status, data = df) } wilcox_pval \u0026lt;- function(df){ wilcox.test(abund ~ Status, data = df)$p.value } #Create nested data frames by OTU and loop over each using map wilcox_results \u0026lt;- ps_wilcox %\u0026gt;% gather(key = OTU, value = abund, -Status) %\u0026gt;% group_by(OTU) %\u0026gt;% nest() %\u0026gt;% mutate(wilcox_test = map(data, wilcox_model), p_value = map(data, wilcox_pval)) #Show results head(wilcox_results) ## # A tibble: 6 x 4 ## OTU data wilcox_test p_value ## \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 OTU1 \u0026lt;tibble [84 x 2]\u0026gt; \u0026lt;S3: htest\u0026gt; \u0026lt;dbl [1]\u0026gt; ## 2 OTU2 \u0026lt;tibble [84 x 2]\u0026gt; \u0026lt;S3: htest\u0026gt; \u0026lt;dbl [1]\u0026gt; ## 3 OTU3 \u0026lt;tibble [84 x 2]\u0026gt; \u0026lt;S3: htest\u0026gt; \u0026lt;dbl [1]\u0026gt; ## 4 OTU4 \u0026lt;tibble [84 x 2]\u0026gt; \u0026lt;S3: htest\u0026gt; \u0026lt;dbl [1]\u0026gt; ## 5 OTU5 \u0026lt;tibble [84 x 2]\u0026gt; \u0026lt;S3: htest\u0026gt; \u0026lt;dbl [1]\u0026gt; ## 6 OTU6 \u0026lt;tibble [84 x 2]\u0026gt; \u0026lt;S3: htest\u0026gt; \u0026lt;dbl [1]\u0026gt; head(wilcox_results$data[[1]]) ## # A tibble: 6 x 2 ## Status abund ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Chronic Fatigue 1.29 ## 2 Control 5.81 ## 3 Control 5.62 ## 4 Chronic Fatigue 6.23 ## 5 Chronic Fatigue 9.47 ## 6 Control 7.43 wilcox_results$wilcox_test[[1]] ## ## Wilcoxon rank sum test ## ## data: abund by Status ## W = 1172, p-value = 0.006066 ## alternative hypothesis: true location shift is not equal to 0 wilcox_results$p_value[[1]] ## [1] 0.006066387 Here we can see that we have a tibble where:\n each OTU is a row the data column contains a tibble for each OTU that contains the CLR abundance and Status fields (i.e. seperate data.frame for each OTU) the wilcox_test column contains the results of each Wilcoxon test the p_value column contains the extracted p-value for each test  \nNow we will unnest the results, grab the OTU names and p-values, add the taxonomic labels, and calculate the FDR adjusted p-values.\n#Unnesting wilcox_results \u0026lt;- wilcox_results %\u0026gt;% dplyr::select(OTU, p_value) %\u0026gt;% unnest() head(wilcox_results) ## # A tibble: 6 x 2 ## OTU p_value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 OTU1 0.00607 ## 2 OTU2 0.0686 ## 3 OTU3 0.830 ## 4 OTU4 0.0130 ## 5 OTU5 0.419 ## 6 OTU6 0.258 #Adding taxonomic labels taxa_info \u0026lt;- data.frame(tax_table(ps_clr)) taxa_info \u0026lt;- taxa_info %\u0026gt;% rownames_to_column(var = \u0026quot;OTU\u0026quot;) #Computing FDR corrected p-values wilcox_results \u0026lt;- wilcox_results %\u0026gt;% full_join(taxa_info) %\u0026gt;% arrange(p_value) %\u0026gt;% mutate(BH_FDR = p.adjust(p_value, \u0026quot;BH\u0026quot;)) %\u0026gt;% filter(BH_FDR \u0026lt; 0.05) %\u0026gt;% dplyr::select(OTU, p_value, BH_FDR, everything()) ## Joining, by = \u0026quot;OTU\u0026quot; #Printing results print.data.frame(wilcox_results)  ## OTU p_value BH_FDR Kingdom Phylum Class ## 1 OTU48 1.893126e-05 0.002612514 Bacteria Firmicutes Clostridia ## 2 OTU38 4.168412e-05 0.002876205 Bacteria Firmicutes Clostridia ## 3 OTU44 2.750125e-04 0.012650574 Bacteria Firmicutes Clostridia ## 4 OTU61 1.217944e-03 0.038379997 Bacteria Firmicutes Clostridia ## 5 OTU104 1.390580e-03 0.038379997 Bacteria Firmicutes Clostridia ## 6 OTU115 1.804359e-03 0.040427044 Bacteria Firmicutes Clostridia ## 7 OTU83 2.050647e-03 0.040427044 Bacteria Firmicutes Clostridia ## 8 OTU8 2.719699e-03 0.041702048 Bacteria Firmicutes Clostridia ## 9 OTU123 2.719699e-03 0.041702048 Bacteria Firmicutes Erysipelotrichi ## Order Family Genus Species ## 1 Clostridiales Lachnospiraceae Coprococcus \u0026lt;NA\u0026gt; ## 2 Clostridiales Ruminococcaceae Oscillospira \u0026lt;NA\u0026gt; ## 3 Clostridiales Lachnospiraceae [Ruminococcus] \u0026lt;NA\u0026gt; ## 4 Clostridiales Lachnospiraceae \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 5 Clostridiales Lachnospiraceae Blautia producta ## 6 Clostridiales [Mogibacteriaceae] \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 7 Clostridiales Lachnospiraceae Blautia \u0026lt;NA\u0026gt; ## 8 Clostridiales Lachnospiraceae [Ruminococcus] \u0026lt;NA\u0026gt; ## 9 Erysipelotrichales Erysipelotrichaceae Coprobacillus \u0026lt;NA\u0026gt; Here we see that we have several Clostridiales organisms identified as differentially abundant. Next, we might use bootstrap resampling to see how often these results replicated in subsets of the data and calculate a measure of effect size. However, we will not do that here. Instead we will take a look at another approach that uses the Wilcoxon test on the CLR transformed data with some improvements in the treatment of zero values and presentation of effect size.\n\nANOVA-like differential expression (ALDEx2) is a popular CoDA method for differential abundance testing. ALDEx2 can be run via a single command; however, there are several steps that are occurring in the background. At a high-level, the steps include:\n Generate a large number (here n=128) of posterior probabilities for the observance of each taxon (i.e. output many data.frames where the counts have been converted to proportions). This is done by Monte-Carlo sampling from a Dirichlet distribution with a small non-zero prior to deal with zeros. The total read count therefore only contributes to the precision of the proportions. Apply the centered log-ratio transformation to each instance. Apply the Wilcoxon test to each taxon for each simulated instance. Estimate the effect size as the difference between conditions divided by the maximum difference within conditions averaging over all instances. Scaling the between group difference by the maximum within group difference gives us a standardized effect size measure. Obtain the expected p-values for each taxon by averaging over all instances. Apply the BH-FDR correction to control the false positive rate.  For a more through explanation see the ALDEx2 Bioconductor vignette.\nLets give it a try.\n#Run ALDEx2 aldex2_da \u0026lt;- ALDEx2::aldex(data.frame(phyloseq::otu_table(ps)), phyloseq::sample_data(ps)$Status, test=\u0026quot;t\u0026quot;, effect = TRUE, denom=\u0026quot;iqlr\u0026quot;) ## aldex.clr: generating Monte-Carlo instances and clr values ## operating in serial mode ## computing iqlr centering ## aldex.ttest: doing t-test ## aldex.effect: calculating effect sizes #Plot effect sizes ALDEx2::aldex.plot(aldex2_da, type=\u0026quot;MW\u0026quot;, test=\u0026quot;wilcox\u0026quot;, called.cex = 1, cutoff = 0.05) The output highlights the various steps for the ALDEx2 workflow. The interquartile log-ratio (iqlr) centering uses as the basis for the CLR transform the set of features that have variance values that fall between the first and third quartiles for all features in all groups in the dataset. This provides results that are more robust to asymmetric features between groups.\nThe effect size plot shows the median log2 fold difference by the median log2 dispersion. This is a measure of the effect size by the variability. Differentially abundant taxon will be those where the difference most exceeds the dispersion. Points toward the top of the figure are more abundant in CF samples while those towards the bottom are more abundant in healthy controls. Taxa with BH-FDR corrected p-values are shown in red. However, the authors state that:\n We prefer to use the effect size whenever possible rather than statistical significance since an effect size tells the scientist what they want to know—“what is reproducibly different between groups”; this is emphatically not something that P values deliver. Now we will print the output with the taxonomic classifications appended. WE use the FDR p-values here to facilitate the comparison with the results from Wilcoxon test ran outside of ALDEx2.\n #Clean up presentation sig_aldex2 \u0026lt;- aldex2_da %\u0026gt;% rownames_to_column(var = \u0026quot;OTU\u0026quot;) %\u0026gt;% filter(wi.eBH \u0026lt; 0.05) %\u0026gt;% arrange(effect, wi.eBH) %\u0026gt;% dplyr::select(OTU, diff.btw, diff.win, effect, wi.ep, wi.eBH) sig_aldex2 \u0026lt;- left_join(sig_aldex2, taxa_info) ## Joining, by = \u0026quot;OTU\u0026quot; sig_aldex2 ## OTU diff.btw diff.win effect wi.ep wi.eBH Kingdom ## 1 OTU8 -2.307035 5.522587 -0.3839105 0.0015096450 0.039837292 Bacteria ## 2 OTU48 3.639216 6.528139 0.5283635 0.0025403967 0.042007768 Bacteria ## 3 OTU44 3.541267 6.324375 0.5296702 0.0013725146 0.031600961 Bacteria ## 4 OTU38 3.277257 4.696329 0.6206553 0.0000348666 0.004124241 Bacteria ## Phylum Class Order Family Genus ## 1 Firmicutes Clostridia Clostridiales Lachnospiraceae [Ruminococcus] ## 2 Firmicutes Clostridia Clostridiales Lachnospiraceae Coprococcus ## 3 Firmicutes Clostridia Clostridiales Lachnospiraceae [Ruminococcus] ## 4 Firmicutes Clostridia Clostridiales Ruminococcaceae Oscillospira ## Species ## 1 \u0026lt;NA\u0026gt; ## 2 \u0026lt;NA\u0026gt; ## 3 \u0026lt;NA\u0026gt; ## 4 \u0026lt;NA\u0026gt; Here we see that again that several Clostridiales organisms are identified as differentially abundant. Consistent with the results of running the Wilcoxon test outside of ALDEx2, we see that OTU48, OTU38, OTU44, and OTU8 are listed as differentially abundant. The others do not reach the FDR cut-off used here; although, they likely have “largish” effect sizes. Try and see if you can obtain these values. The reason for the discrepancy is hard to discern, but may be related to differences in the use of the CLR basis (geometric mean of all taxa versus the IQLR) and/or the use of the Bayesian resampling with a non-zero prior.\nOften, if I consider performing DA testing, I will run several models and focus on the intersection of OTUs and try to gain some insight into how the different normalization and/or models many be influencing the results.\nThere are MANY other approaches that can be used to attempt to identify differently abundant taxa. Some that are popular, or that I find interesting, and can be implemented in R include:\n Count Regression for Correlated Observations with the Beta-binomial (corncob) MicrobiomeDDA DESeq2 Analysis of Composition of Microbiomes (ANCOM)  Outside of R, a recently developed approach using multinomial regression via tensorflow and differential ranking looks promising.\n\n Prediction As discussed by Dr. Haslam in the first lecture, the majority of clinical microbiome studies, conducted to date, have been correlative or focused on predicting outcomes using taxonomic abundances as the feature set. The predictive utility of the human microbiome in health and disease is of great interest and numerous studies have reported the ability to predict outcomes from metagenomic data. For example, here are links to three studies suggesting taxonomic profiles in fecal samples may predict the occurrence of colorectal cancer (1, 2, 3).\nAs with differential abundance testing, there are many models or statistical learning approaches that can be applied to metagenomic data for the purpose of predicting an outcome. For binary outcomes, generating predicted probabilities for the outcome of interest using generalized linear models (GLMs) is one approach. Machine learning approaches have also been used extensively in microbiome research; however, these approaches may likely require much larger datasets than they have typically been trained on if our goal is reproducible results (the same likely goes for most studies using GLMs, etc.).\nOne challenge we face when building a predictive model from metagenomic data is that we often have more features (taxon) than we have samples. For example, if we are working with microbial strains we might have more than 10,000 features to consider. One way to define high-dimensional data is when p \u0026gt; n, where: p = number of features and n = the number of samples. In these instances, one approach forward to reduce the dimensionality of the data. We did this earlier when we used PCA to extract the first two PCs that explained the largest fraction of variably in our data. Using a subset of the PCs as predictors in a GLM is known as principal components regression. We will give this approach a try below. Another is to include all the features as predictors, but to shrink their effects towards zero (or sometimes shrink them entirely out of the model). These approaches go by names such as ridge regression, LASSO, elastic nets, etc. Bayesian models with skeptical priors also can work well here. We will use a form of penalization on the principal components regression model below to highlight this approach and address potential overfitting even with just three PCs at this sample size (which is likely too small for robust prediction). A helpful guide to think about how many features or samples one might require to develop a predictive model is to consider how much overfitting you are willing to accept. Here are links to two excellent papers describing sample size determinations for continuous and binary outcomes in predictive modeling.\nWe will also examine a CoDA greedy stepwise selection model using balances that I think is a lot of fun…and very user-friendly.\nFor those interested in general resources for prediction modeling I recommend:\n Frank Harrell’s Regression Modeling Stratagies website and textbook Ewout Steyerberg’s Clinical Prediction Models textbook for a bit more introductory text Max Kuhn’s Applied Predictive Modeling textbook James, Witten, Hastie, and Tibshirani’s An Introduction to Statistical Learning  First we will create a data.frame that contains the Status and the first 3 PCs from the centered-log ratio transformed abundance table we generated before. We will then plot the unconditional association for each PC with the outcome of CF versus control.\n#Generate data.frame clr_pcs \u0026lt;- data.frame( \u0026quot;pc1\u0026quot; = ord_clr$CA$u[,1], \u0026quot;pc2\u0026quot; = ord_clr$CA$u[,2], \u0026quot;pc3\u0026quot; = ord_clr$CA$u[,3], \u0026quot;Status\u0026quot; = phyloseq::sample_data(ps_clr)$Status ) clr_pcs$Status_num \u0026lt;- ifelse(clr_pcs$Status == \u0026quot;Control\u0026quot;, 0, 1) head(clr_pcs) ## pc1 pc2 pc3 Status ## ERR1331793 0.02850343 -0.07709724 0.0938970408 Chronic Fatigue ## ERR1331872 -0.08156129 0.14193568 0.1155088427 Control ## ERR1331819 -0.19356039 -0.08436341 -0.1048722096 Control ## ERR1331794 -0.04193714 0.09705602 0.0110912849 Chronic Fatigue ## ERR1331851 0.09994410 0.05534786 -0.0005008101 Chronic Fatigue ## ERR1331834 -0.15577774 0.02921040 -0.0204667015 Control ## Status_num ## ERR1331793 1 ## ERR1331872 0 ## ERR1331819 0 ## ERR1331794 1 ## ERR1331851 1 ## ERR1331834 0 #Specify a datadist object (for rms) dd \u0026lt;- datadist(clr_pcs) options(datadist = \u0026quot;dd\u0026quot;) #Plot the unconditional associations a \u0026lt;- ggplot(clr_pcs, aes(x = pc1, y = Status_num)) + Hmisc::histSpikeg(Status_num ~ pc1, lowess = TRUE, data = clr_pcs) + labs(x = \u0026quot;\\nPC1\u0026quot;, y = \u0026quot;Pr(Chronic Fatigue)\\n\u0026quot;) b \u0026lt;- ggplot(clr_pcs, aes(x = pc2, y = Status_num)) + Hmisc::histSpikeg(Status_num ~ pc2, lowess = TRUE, data = clr_pcs) + labs(x = \u0026quot;\\nPC2\u0026quot;, y = \u0026quot;Pr(Chronic Fatigue)\\n\u0026quot;) c \u0026lt;- ggplot(clr_pcs, aes(x = pc3, y = Status_num)) + Hmisc::histSpikeg(Status_num ~ pc3, lowess = TRUE, data = clr_pcs) + labs(x = \u0026quot;\\nPC3\u0026quot;, y = \u0026quot;Pr(Chronic Fatigue)\\n\u0026quot;) cowplot::plot_grid(a, b, c, nrow = 2, ncol = 2, scale = .9, labels = \u0026quot;AUTO\u0026quot;) We see that we have the potential for some non-linear associations. Professor Harrell recommends that it is generally a good idea to assume some level of complexity since the penalty for allowing for a non-linear fit, when the association is in fact linear, is much less than when assuming linearity when the association is non-linear (i.e. you fit a straight line through a u-shaped curve). His rms package, along with the tidyverse, are the two packages I use most often and allows us to model this type of complexity easily using restricted cubic splines. These are a set of highly flexible, smoothly joined, piecewise polynomials entered for each variable. The number and placement of the knots helps control the flexibility. We will allow three knots for each term. However, this results in an additional 3 (6 total) model degrees of freedom…but we will shrink this down.\nWe first fit the full model and then perform a grid search to identify the optimum value for the penalty. We can also allow the penalty to differ for the simple and complex (i.e. nonlinear or interactions) terms. This is helpful if we want to allow for complexity, but down weight its impact. This would kind of be like adding more restrictive priors to the non-linear terms in a Bayesian model. We then plot the penalized log odds.\n#Fit full model with splines (3 knots each) m1 \u0026lt;- rms::lrm(Status_num ~ rcs(pc1, 3) + rcs(pc2, 3) + rcs(pc3, 3), data = clr_pcs, x = TRUE, y = TRUE) #Grid search for penalties pentrace(m1, list(simple = c(0, 1, 2), nonlinear = c(0, 100, 200))) ## ## Best penalty: ## ## simple nonlinear df ## 1 200 2.783027 ## ## simple nonlinear df aic bic aic.c ## 0 0 6.000000 23.10845 8.523552 22.01754 ## 0 100 3.049209 28.21043 20.798359 27.90157 ## 1 100 2.810152 28.38363 21.552668 28.11659 ## 2 100 2.641219 28.11811 21.697792 27.87875 ## 0 200 3.024831 28.24577 20.892958 27.94131 ## 1 200 2.783027 28.42060 21.655570 28.15810 ## 2 200 2.611196 28.15166 21.804324 27.91706 pen_m1 \u0026lt;- update(m1, penalty = list(simple = 1, nonlinear = 200)) pen_m1 ## Logistic Regression Model ## ## rms::lrm(formula = Status_num ~ rcs(pc1, 3) + rcs(pc2, 3) + rcs(pc3, ## 3), data = clr_pcs, x = TRUE, y = TRUE, penalty = list(simple = 1, ## nonlinear = 200)) ## ## ## Penalty factors ## ## simple nonlinear interaction nonlinear.interaction ## 1 200 200 200 ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 84 LR chi2 33.99 R2 0.421 C 0.848 ## 0 37 d.f. 2.783 g 1.759 Dxy 0.695 ## 1 47 Pr(\u0026gt; chi2)\u0026lt;0.0001 gr 5.807 gamma 0.695 ## max |deriv| 1e-12 Penalty 2.34 gp 0.322 tau-a 0.347 ## Brier 0.159 ## ## Coef S.E. Wald Z Pr(\u0026gt;|Z|) Penalty Scale ## Intercept 0.3458 0.2852 1.21 0.2254 0.0000 ## pc1 11.6489 2.8714 4.06 \u0026lt;0.0001 0.1098 ## pc1\u0026#39; 0.1202 0.9287 0.13 0.8970 1.0715 ## pc2 6.4946 2.5132 2.58 0.0098 0.1098 ## pc2\u0026#39; -0.0015 0.7643 0.00 0.9984 1.2987 ## pc3 -3.8538 2.5659 -1.50 0.1331 0.1098 ## pc3\u0026#39; 0.0259 1.0080 0.03 0.9795 0.9856 ##  #Plot log odds ggplot(Predict(pen_m1)) We can see from the value of the penalties and the resultant log odds that the conditional associations are quite linear. However, we will leave in the cubic spline terms to fully account for the degrees of freedom we entertained in the model building process. The optimal penalties were 1 for the simple and 200 for the non-linear terms (higher is better for the corrected AIC) and the effective degrees of freedom shrunk to 2.78. We won’t interpret the coefficients here since we purposefully biased them towards zero.\nThe Brier score is 0.16 and provides a measure of the mean squared difference between the predicted probabilities and actual outcomes. Thus, it is a quadratic proper scoring rule. The c-statistic is analogous to the area under the receiver operating characteristic curve and is a measure of rank discrimination. Here it is c = 0.85.\n\nNo we will perform bootstrap resampling to obtain an out-of-sample estimate of model performance. Here is a link describing this in greater detail.\n#Obtain optimism corrected estimates (val \u0026lt;- rms::validate(pen_m1)) ## index.orig training test optimism index.corrected n ## Dxy 0.6952 0.7248 0.6817 0.0431 0.6521 40 ## R2 0.4206 0.4536 0.4295 0.0240 0.3965 40 ## Intercept 0.0000 0.0000 -0.0250 0.0250 -0.0250 40 ## Slope 1.0000 1.0000 1.0265 -0.0265 1.0265 40 ## Emax 0.0000 0.0000 0.0100 0.0100 0.0100 40 ## D 0.3927 0.4045 0.3749 0.0297 0.3630 40 ## U -0.0238 -0.0238 -0.0073 -0.0165 -0.0073 40 ## Q 0.4165 0.4283 0.3822 0.0461 0.3704 40 ## B 0.1589 0.1481 0.1647 -0.0166 0.1755 40 ## g 1.7591 1.9083 1.9158 -0.0075 1.7666 40 ## gp 0.3218 0.3287 0.3363 -0.0076 0.3293 40 #Compute corrected c-statistic (c_opt_corr \u0026lt;- 0.5 * (val[1, 5] + 1)) ## [1] 0.8260443 #Plot calibration cal \u0026lt;- rms::calibrate(pen_m1, B = 200) plot(cal) ## ## n=84 Mean absolute error=0.01 Mean squared error=0.00018 ## 0.9 Quantile of absolute error=0.024 #Output pred. probs head(predict(pen_m1, type =\u0026quot;fitted\u0026quot;)) ## ERR1331793 ERR1331872 ERR1331819 ERR1331794 ERR1331851 ERR1331834 ## 0.4560689 0.4689260 0.1137757 0.6098891 0.8683044 0.2314747 We can see here that the Brier score is only mildly increased, and the c-statistic mildly decreased with repeated resampling. The calibration curve shows that the predictions are near the ideal across the range of predicted values. All-in-all this suggests we may expect to be able to predict patients with chronic fatigue from healthy controls with reasonable accuracy in a new sample of patients drawn from a similar population using just the three top PCs.\n\nNow we will quickly show selbal as an alternaitve. From the documentation selbal is described as:\n selbal is an R package for selection of balances in microbiome compositional data. As described in Rivera-Pinto et al. 2018 Balances: a new perspective for microbiome analysis https://doi.org/10.1101/219386, selbal implements a forward-selection method for the identification of two groups of taxa whose relative abundance, or balance, is associated with the response variable of interest. It requires much less typing…so let’s give it a go. This approach is computationally expensive (especially with larger datasets). So below we only use 1 repeat of 5-fold cross-validation to tune the selections. In practice, we would want to turn these numbers up to get better estimates. We will also aggregate the taxa to the family-level to speed up the computation.\n #Agglomerate taxa (ps_family \u0026lt;- phyloseq::tax_glom(ps, \u0026quot;Family\u0026quot;)) ## phyloseq-class experiment-level object ## otu_table() OTU Table: [ 30 taxa and 84 samples ] ## sample_data() Sample Data: [ 84 samples by 23 sample variables ] ## tax_table() Taxonomy Table: [ 30 taxa by 7 taxonomic ranks ] ## phy_tree() Phylogenetic Tree: [ 30 tips and 29 internal nodes ] ## refseq() DNAStringSet: [ 30 reference sequences ] phyloseq::taxa_names(ps_family) \u0026lt;- phyloseq::tax_table(ps_family)[, \u0026quot;Family\u0026quot;] #Run selbal cv_sebal \u0026lt;- selbal::selbal.cv(x = data.frame(t(data.frame(phyloseq::otu_table(ps_family)))), y = phyloseq::sample_data(ps_family)$Status, n.fold = 5, n.iter = 1)  ## ## ## ############################################################### ## STARTING selbal.cv FUNCTION ## ############################################################### ## ## #-------------------------------------------------------------# ## # ZERO REPLACEMENT . . . ## Loading required package: MASS ## ## Attaching package: \u0026#39;MASS\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## select ## Loading required package: NADA ## ## Attaching package: \u0026#39;NADA\u0026#39; ## The following object is masked from \u0026#39;package:IRanges\u0026#39;: ## ## cor ## The following object is masked from \u0026#39;package:S4Vectors\u0026#39;: ## ## cor ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## cor ## Loading required package: truncnorm ## Loading required package: miscF ## Loading required package: R2jags ## Loading required package: rjags ## Loading required package: coda ## Warning: package \u0026#39;coda\u0026#39; was built under R version 3.6.1 ## Linked to JAGS 4.3.0 ## Loaded modules: basemod,bugs ## ## Attaching package: \u0026#39;R2jags\u0026#39; ## The following object is masked from \u0026#39;package:coda\u0026#39;: ## ## traceplot ## ## Attaching package: \u0026#39;miscF\u0026#39; ## The following object is masked from \u0026#39;package:Hmisc\u0026#39;: ## ## rMultinom ## ## , . . . FINISHED. ## #-------------------------------------------------------------# ## ## #-------------------------------------------------------------# ## # Starting the cross - validation procedure . . . ## Warning in e$fun(obj, substitute(ex), parent.frame(), e$data): already ## exporting variable(s): logit.acc ## ## . . . finished. ## #-------------------------------------------------------------# ## ############################################################### ## ## The optimal number of variables is: 2 ## Setting levels: control = Control, case = Chronic Fatigue ## Setting direction: controls \u0026lt; cases ## Setting levels: control = Control, case = Chronic Fatigue ## Setting direction: controls \u0026lt; cases ## ## Attaching package: \u0026#39;gridExtra\u0026#39; ## The following object is masked from \u0026#39;package:Biobase\u0026#39;: ## ## combine ## The following object is masked from \u0026#39;package:BiocGenerics\u0026#39;: ## ## combine ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combine ## ## ## ############################################################### ## . . . FINISHED. ## ############################################################### #plot/print results cv_sebal$accuracy.nvar plot.new() grid.draw(cv_sebal$global.plot) Here we can see that the cross-validation selected the two balance object as having “among” the best rank-discrimination. It selected the balance with erysipelotrichaceae in the numerator and bifidobacteriaceae in the denominator. So a higher relative abundance of erysipelotrichaceae to bifidobacteriaceae was among the most informative balances. The AUC was 0.77, but as low as AUC = 0.68 with 1 repeat of 5 fold cross-validation.\nGive the model a try on the full ps object on your own. It should run in ~5 min on a standard laptop. How does the performance compare? Would you expect these results to be as reproducible as the GLM we fit? Why?\n\n That concludes this session.  ","date":1564272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564367153,"objectID":"f32f90b3d5b1c22d5d3b09572c0ed2f6","permalink":"/post/introduction-to-the-statistical-analysis-of-microbiome-data-in-r/","publishdate":"2019-07-28T00:00:00Z","relpermalink":"/post/introduction-to-the-statistical-analysis-of-microbiome-data-in-r/","section":"post","summary":"This post is also from the Introduction to Metagenomics Summer Workshop and provides a quick introduction to some common analytic methods used to analyze microbiome data. I thought it might be of interest to a broader audience so decided to post it here.\n\nThe goal of this session is to provide you with a high-level introduction to some common analytic methods used to analyze microbiome data. It will also serve to introduce you several popular R packages developed specifically for microbiome data analysis.","tags":["Microbiome","R","Data Analysis"],"title":"Introduction to the Statistical Analysis of Microbiome Data in R","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]